# This file is automatically generated by pyo3_stub_gen
# ruff: noqa: E501, F401

import os
import pathlib
import typing
from . import variable_names
from enum import Enum, auto


# Autogenerated types for recursions etc.
type ResolvedValueType = ResolvedValueTypeOut
type ResolvedValueTypeIn = tuple[DictionaryLanguage, int] | tuple[Domain, int] | tuple[Register, int] | tuple[GrammaticalGender, int] | tuple[PartOfSpeech, int] | tuple[Region, int] | tuple[GrammaticalNumber, int] | tuple[PartOfSpeechTag, int] | tuple[str, int] | tuple[int, int]
type ResolvedValueTypeOut = tuple[DictionaryLanguage, int] | tuple[Domain, int] | tuple[Register, int] | tuple[GrammaticalGender, int] | tuple[PartOfSpeech, int] | tuple[Region, int] | tuple[GrammaticalNumber, int] | tuple[PartOfSpeechTag, int] | tuple[str, int] | tuple[int, int]
type PyExprValueSingle = str | float | int | bool | None | list[PyExprValueSingle]
type SearchTypeUnionType = SearchType | typing.Literal["a", "auto", "e", "exact", "c", "contains", "start", "starts_with", "end", "ends_with", "r", "reg", "regex", "h", "ham", "hamming", "l", "lev", "levensthein", "o", "osa", "osa_dist", "osa_distance", "l+n", "lev_norm", "lev_normalized", "dl", "dam_lev", "damerau_levensthein", "dl+n", "dam_lev_norm", "damerau_levensthein_norm", "damerau_levensthein_normalized", "j", "jaro", "jw", "jaro_winkler", "s", "soren", "sorensen_dice", "cp", "com_pre", "common_prefix"]
type SearchResultContainerType = tuple[list[str] | dict[str, dict[str, set[str]]], list[str] | dict[str, dict[str, set[str]]]] | tuple[list[str] | dict[str, dict[str, set[str]]], None] | tuple[None, list[str] | dict[str, dict[str, set[str]]]]


class DictMetaCount:
    def full(self) -> dict[Domain | Register | int, int]:
        ...

    def exists(self) -> set[Domain | Register | int]:
        ...

    def __len__(self) -> int:
        ...

    def __getitem__(self, index:Domain | Register | int) -> int:
        ...

    def sum(self) -> int:
        ...

    def __str__(self) -> str:
        ...

    def as_dict(self) -> dict[Domain | Register | int, int]:
        r"""
        Converts the count to a normal dict
        """
        ...


class DictMetaCounts:
    def a(self) -> DictMetaCount:
        ...

    def b(self) -> DictMetaCount:
        ...

    def __len__(self) -> int:
        ...

    def __getitem__(self, index:Domain | Register | int) -> tuple[int, int]:
        ...

    def sum(self) -> tuple[int, int]:
        ...

    def __str__(self) -> str:
        ...

    def as_dicts(self) -> tuple[dict[Domain | Register | int, int], dict[Domain | Register | int, int]]:
        r"""
        Returns a tuple of dicts, representing (a, b) as hashmaps.
        """
        ...


class DictMetaModel:
    def __new__(cls,capacity = ...): ...
    def __str__(self) -> str:
        ...

    def to_list(self) -> list[PyDictMetaVector]:
        ...


class DictionaryLanguageDirection:
    lang_a: DictionaryLanguage
    lang_b: DictionaryLanguage
    def __new__(cls,lang_a:DictionaryLanguage, lang_b:DictionaryLanguage): ...
    def __contains__(self, other:DictionaryLanguage) -> bool:
        r"""
        Returns true if this contains [other] as any position
        """
        ...

    def __getitem__(self, language_kind:LanguageMarker) -> DictionaryLanguage:
        ...

    def invert(self) -> DictionaryLanguageDirection:
        r"""
        Returns an inverted variant
        """
        ...

    def __invert__(self) -> DictionaryLanguageDirection:
        r"""
        Returns an inverted variant
        """
        ...

    def __neg__(self) -> DictionaryLanguageDirection:
        r"""
        Returns an inverted variant
        """
        ...

    def is_direction_in(self, lang_a:DictionaryLanguage, lang_b:DictionaryLanguage) -> bool:
        r"""
        Returns true if this points from [lang_a] to [lang_b]
        """
        ...


class LanguageHint:
    r"""
    A hint for the language used.
    """
    def __new__(cls,language:str): ...
    def __eq__(self, other:LanguageHint) -> bool:
        ...

    def __hash__(self) -> int:
        ...

    def __repr__(self) -> str:
        ...

    def __str__(self) -> str:
        ...


class Len:
    voc_a: int
    voc_b: int
    map_a_to_b: int
    map_b_to_a: int
    def __str__(self) -> str:
        ...

    def diff(self, other:Len) -> Len:
        ...


class LoadedMetadataEx:
    r"""
    The metadata for a specific word.
    
    It contains general metadata, usually set by users and associated metadata,
    extracted from the original dictionaries.
    """
    languages_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    domains_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    registers_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    genders_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    pos_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    pos_tag_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    regions_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    numbers_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    internal_ids_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    inflected_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    abbreviations_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    unaltered_vocabulary_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    look_at_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    ids_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    outgoing_ids_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    original_entry_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    contextual_informations_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    unclassified_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    synonyms_py: tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]
    def __new__(cls,values,additional_dictionaries = ...): ...
    def dict_meta_vector(self) -> PyDictMetaVector:
        r"""
        Returns a domain vector. The returned DictMetaVector consists of the counts of the
        single topics [Domain] and [Register].
        """
        ...

    def meta_count(self) -> DictMetaCount:
        r"""
        Returns a count element.
        """
        ...

    def get_languages_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def get_domains_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def get_registers_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def get_genders_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def get_pos_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def get_pos_tag_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def get_regions_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def get_numbers_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def get_internal_ids_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def get_inflected_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def get_abbreviations_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def get_unaltered_vocabulary_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def get_look_at_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def get_ids_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def get_outgoing_ids_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def get_original_entry_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def get_contextual_informations_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def get_unclassified_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def get_synonyms_single(self, dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the general value for this specific field.
        If a dictionary name is provided, it returns the value of this specific dictionary.
        """
        ...

    def __str__(self) -> str:
        ...

    def __repr__(self) -> str:
        ...

    def get_single_field(self, field,dictionary) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for a specific field. If a dictionary name is provided,
        it returns the values of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_field(self, field) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the metadata of this specific field.
        """
        ...

    def topic_vector(self) -> PyDictMetaVector:
        ...

    def associated_dictionaries(self) -> set[str]:
        r"""
        Returns the associated dictionaries.
        """
        ...

    def as_dict(self) -> dict[MetaField, tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]]:
        r"""
        Returns the metadata as dict.
        """
        ...


class NGramDefinition:
    def __new__(cls,language:LanguageHint, ngram_size:int, file_max:int): ...
    ...

class PyAhoCorasick:
    r"""
    An automaton for searching multiple strings in linear time.
    This is only used to hold the results.
    """
    @staticmethod
    def builder() -> PyAhoCorasickBuilder:
        ...


class PyAhoCorasickBuilder:
    def __new__(cls,): ...
    def build(self, patterns:typing.Sequence[str]) -> PyAhoCorasick:
        r"""
        Build an Aho-Corasick automaton using the configuration set on this
        builder.
        
        A builder may be reused to create more automatons.
        
        # Examples
        
        Basic usage:
        
        ```
        use aho_corasick::{AhoCorasickBuilder, PatternID};
        
        let patterns = &["foo", "bar", "baz"];
        let ac = AhoCorasickBuilder::new().build(patterns).unwrap();
        assert_eq!(
            Some(PatternID::must(1)),
            ac.find("xxx bar xxx").map(|m| m.pattern()),
        );
        ```
        """
        ...

    def match_kind(self, kind) -> PyAhoCorasickBuilder:
        r"""
        Set the desired match semantics.
        
        The default is [`PyMatchKind::Standard`], which corresponds to the match
        semantics supported by the standard textbook description of the
        Aho-Corasick algorithm. Namely, matches are reported as soon as they
        are found. Moreover, this is the only way to get overlapping matches
        or do stream searching.
        
        The other kinds of match semantics that are supported are
        [`PyMatchKind::LeftmostFirst`] and [`PyMatchKind::LeftmostLongest`]. The
        former corresponds to the match you would get if you were to try to
        match each pattern at each position in the haystack in the same order
        that you give to the automaton. That is, it returns the leftmost match
        corresponding to the earliest pattern given to the automaton. The
        latter corresponds to finding the longest possible match among all
        leftmost matches.
        
        For more details on match semantics, see the [documentation for
        `MatchKind`](MatchKind).
        
        Note that setting this to [`PyMatchKind::LeftmostFirst`] or
        [`PyMatchKind::LeftmostLongest`] will cause some search routines on
        [`PyAhoCorasick`] to return an error (or panic if you're using the
        infallible API). Notably, this includes stream and overlapping
        searches.
        
        # Examples
        
        In these examples, we demonstrate the differences between match
        semantics for a particular set of patterns in a specific order:
        `b`, `abc`, `abcd`.
        
        Standard semantics:
        
        ```plaintext
        use ldatranslate::py::tokenizer::{PyAhoCorasick, PyMatchKind};
        
        let patterns = &["b", "abc", "abcd"];
        let haystack = "abcd";
        
        let ac = PyAhoCorasick::builder()
            .match_kind(PyMatchKind::Standard) // default, not necessary
            .build(patterns)
            .unwrap();
        let mat = ac.find(haystack).expect("should have a match");
        assert_eq!("b", &haystack[mat.start()..mat.end()]);
        ```
        
        Leftmost-first semantics:
        
        ```plaintext
        use ldatranslate::py::tokenizer::{PyAhoCorasick, PyMatchKind};
        
        let patterns = &["b", "abc", "abcd"];
        let haystack = "abcd";
        
        let ac = PyAhoCorasick::builder()
            .match_kind(PyMatchKind::LeftmostFirst)
            .build(patterns)
            .unwrap();
        let mat = ac.find(haystack).expect("should have a match");
        assert_eq!("abc", &haystack[mat.start()..mat.end()]);
        ```
        
        Leftmost-longest semantics:
        
        ```plaintext
        use ldatranslate::py::tokenizer::{PyAhoCorasick, PyMatchKind};
        
        let patterns = &["b", "abc", "abcd"];
        let haystack = "abcd";
        
        let ac = PyAhoCorasick::builder()
            .match_kind(PyMatchKind::LeftmostLongest)
            .build(patterns)
            .unwrap();
        let mat = ac.find(haystack).expect("should have a match");
        assert_eq!("abcd", &haystack[mat.start()..mat.end()]);
        ```
        """
        ...

    def start_kind(self, kind) -> PyAhoCorasickBuilder:
        r"""
        Sets the starting state configuration for the automaton.
        
        Every Aho-Corasick automaton is capable of having two start states: one
        that is used for unanchored searches and one that is used for anchored
        searches. Some automatons, like the NFAs, support this with almost zero
        additional cost. Other automatons, like the DFA, require two copies of
        the underlying transition table to support both simultaneously.
        
        Because there may be an added non-trivial cost to supporting both, it
        is possible to configure which starting state configuration is needed.
        
        Indeed, since anchored searches tend to be somewhat more rare,
        _only_ unanchored searches are supported by default. Thus,
        [`PyStartKind::Unanchored`] is the default.
        
        Note that when this is set to [`PyStartKind::Unanchored`], then
        running an anchored search will result in an error (or a panic
        if using the infallible APIs). Similarly, when this is set to
        [`PyStartKind::Anchored`], then running an unanchored search will
        result in an error (or a panic if using the infallible APIs). When
        [`PyStartKind::Both`] is used, then both unanchored and anchored searches
        are always supported.
        
        Also note that even if an `PyAhoCorasick` searcher is using an NFA
        internally (which always supports both unanchored and anchored
        searches), an error will still be reported for a search that isn't
        supported by the configuration set via this method. This means,
        for example, that an error is never dependent on which internal
        implementation of Aho-Corasick is used.
        
        # Example: anchored search
        
        This shows how to build a searcher that only supports anchored
        searches:
        
        ```
        use aho_corasick::{
            AhoCorasick, Anchored, Input, Match, MatchKind, StartKind,
        };
        
        let ac = AhoCorasick::builder()
            .match_kind(MatchKind::LeftmostFirst)
            .start_kind(StartKind::Anchored)
            .build(&["b", "abc", "abcd"])
            .unwrap();
        
        // An unanchored search is not supported! An error here is guaranteed
        // given the configuration above regardless of which kind of
        // Aho-Corasick implementation ends up being used internally.
        let input = Input::new("foo abcd").anchored(Anchored::No);
        assert!(ac.try_find(input).is_err());
        
        let input = Input::new("foo abcd").anchored(Anchored::Yes);
        assert_eq!(None, ac.try_find(input)?);
        
        let input = Input::new("abcd").anchored(Anchored::Yes);
        assert_eq!(Some(Match::must(1, 0..3)), ac.try_find(input)?);
        
        # Ok::<(), Box<dyn std::error::Error>>(())
        ```
        
        # Example: unanchored and anchored searches
        
        This shows how to build a searcher that supports both unanchored and
        anchored searches:
        
        ```
        use aho_corasick::{
            AhoCorasick, Anchored, Input, Match, MatchKind, StartKind,
        };
        
        let ac = AhoCorasick::builder()
            .match_kind(MatchKind::LeftmostFirst)
            .start_kind(StartKind::Both)
            .build(&["b", "abc", "abcd"])
            .unwrap();
        
        let input = Input::new("foo abcd").anchored(Anchored::No);
        assert_eq!(Some(Match::must(1, 4..7)), ac.try_find(input)?);
        
        let input = Input::new("foo abcd").anchored(Anchored::Yes);
        assert_eq!(None, ac.try_find(input)?);
        
        let input = Input::new("abcd").anchored(Anchored::Yes);
        assert_eq!(Some(Match::must(1, 0..3)), ac.try_find(input)?);
        
        # Ok::<(), Box<dyn std::error::Error>>(())
        ```
        """
        ...

    def ascii_case_insensitive(self, yes) -> PyAhoCorasickBuilder:
        r"""
        Enable ASCII-aware case insensitive matching.
        
        When this option is enabled, searching will be performed without
        respect to case for ASCII letters (`a-z` and `A-Z`) only.
        
        Enabling this option does not change the search algorithm, but it may
        increase the size of the automaton.
        
        **NOTE:** It is unlikely that support for Unicode case folding will
        be added in the future. The ASCII case works via a simple hack to the
        underlying automaton, but full Unicode handling requires a fair bit of
        sophistication. If you do need Unicode handling, you might consider
        using the [`regex` crate](https://docs.rs/regex) or the lower level
        [`regex-automata` crate](https://docs.rs/regex-automata).
        
        # Examples
        
        Basic usage:
        
        ```
        use aho_corasick::AhoCorasick;
        
        let patterns = &["FOO", "bAr", "BaZ"];
        let haystack = "foo bar baz";
        
        let ac = AhoCorasick::builder()
            .ascii_case_insensitive(true)
            .build(patterns)
            .unwrap();
        assert_eq!(3, ac.find_iter(haystack).count());
        ```
        """
        ...

    def kind(self, kind = ...) -> PyAhoCorasickBuilder:
        r"""
        Choose the type of underlying automaton to use.
        
        Currently, there are four choices:
        
        * [`PyAhoCorasickKind::NoncontiguousNFA`] instructs the searcher to
        use a [`noncontiguous::NFA`]. A noncontiguous NFA is the fastest to
        be built, has moderate memory usage and is typically the slowest to
        execute a search.
        * [`PyAhoCorasickKind::ContiguousNFA`] instructs the searcher to use a
        [`contiguous::NFA`]. A contiguous NFA is a little slower to build than
        a noncontiguous NFA, has excellent memory usage and is typically a
        little slower than a DFA for a search.
        * [`PyAhoCorasickKind::DFA`] instructs the searcher to use a
        [`dfa::DFA`]. A DFA is very slow to build, uses exorbitant amounts of
        memory, but will typically execute searches the fastest.
        * `None` (the default) instructs the searcher to choose the "best"
        Aho-Corasick implementation. This choice is typically based primarily
        on the number of patterns.
        
        Setting this configuration does not change the time complexity for
        constructing the Aho-Corasick automaton (which is `O(p)` where `p`
        is the total number of patterns being compiled). Setting this to
        [`PyAhoCorasickKind::DFA`] does however reduce the time complexity of
        non-overlapping searches from `O(n + p)` to `O(n)`, where `n` is the
        length of the haystack.
        
        In general, you should probably stick to the default unless you have
        some kind of reason to use a specific Aho-Corasick implementation. For
        example, you might choose `PyAhoCorasickKind::DFA` if you don't care
        about memory usage and want the fastest possible search times.
        
        Setting this guarantees that the searcher returned uses the chosen
        implementation. If that implementation could not be constructed, then
        an error will be returned. In contrast, when `None` is used, it is
        possible for it to attempt to construct, for example, a contiguous
        NFA and have it fail. In which case, it will fall back to using a
        noncontiguous NFA.
        
        If `None` is given, then one may use [`PyAhoCorasickKind::kind`] to determine
        which Aho-Corasick implementation was chosen.
        
        Note that the heuristics used for choosing which `PyAhoCorasickKind`
        may be changed in a semver compatible release.
        """
        ...

    def prefilter(self, yes) -> PyAhoCorasickBuilder:
        r"""
        Enable heuristic prefilter optimizations.
        
        When enabled, searching will attempt to quickly skip to match
        candidates using specialized literal search routines. A prefilter
        cannot always be used, and is generally treated as a heuristic. It
        can be useful to disable this if the prefilter is observed to be
        sub-optimal for a particular workload.
        
        Currently, prefilters are typically only active when building searchers
        with a small (less than 100) number of patterns.
        
        This is enabled by default.
        """
        ...

    def dense_depth(self, depth) -> PyAhoCorasickBuilder:
        r"""
        Set the limit on how many states use a dense representation for their
        transitions. Other states will generally use a sparse representation.
        
        A dense representation uses more memory but is generally faster, since
        the next transition in a dense representation can be computed in a
        constant number of instructions. A sparse representation uses less
        memory but is generally slower, since the next transition in a sparse
        representation requires executing a variable number of instructions.
        
        This setting is only used when an Aho-Corasick implementation is used
        that supports the dense versus sparse representation trade off. Not all
        do.
        
        This limit is expressed in terms of the depth of a state, i.e., the
        number of transitions from the starting state of the automaton. The
        idea is that most of the time searching will be spent near the starting
        state of the automaton, so states near the start state should use a
        dense representation. States further away from the start state would
        then use a sparse representation.
        
        By default, this is set to a low but non-zero number. Setting this to
        `0` is almost never what you want, since it is likely to make searches
        very slow due to the start state itself being forced to use a sparse
        representation. However, it is unlikely that increasing this number
        will help things much, since the most active states have a small depth.
        More to the point, the memory usage increases superlinearly as this
        number increases.
        """
        ...

    def byte_classes(self, yes) -> PyAhoCorasickBuilder:
        r"""
        A debug setting for whether to attempt to shrink the size of the
        automaton's alphabet or not.
        
        This option is enabled by default and should never be disabled unless
        one is debugging the underlying automaton.
        
        When enabled, some (but not all) Aho-Corasick automatons will use a map
        from all possible bytes to their corresponding equivalence class. Each
        equivalence class represents a set of bytes that does not discriminate
        between a match and a non-match in the automaton.
        
        The advantage of this map is that the size of the transition table can
        be reduced drastically from `#states * 256 * sizeof(u32)` to
        `#states * k * sizeof(u32)` where `k` is the number of equivalence
        classes (rounded up to the nearest power of 2). As a result, total
        space usage can decrease substantially. Moreover, since a smaller
        alphabet is used, automaton compilation becomes faster as well.
        
        **WARNING:** This is only useful for debugging automatons. Disabling
        this does not yield any speed advantages. Namely, even when this is
        disabled, a byte class map is still used while searching. The only
        difference is that every byte will be forced into its own distinct
        equivalence class. This is useful for debugging the actual generated
        transitions because it lets one see the transitions defined on actual
        bytes instead of the equivalence classes.
        """
        ...


class PyAlignedArticle:
    article_id: int
    language_hints: list[LanguageHint]
    def __new__(cls,article_id:int, articles:typing.Mapping[str | LanguageHint, PyArticle]): ...
    @staticmethod
    def create(article_id:int, articles:dict[str | LanguageHint, PyArticle] | list[PyArticle]) -> PyAlignedArticle | tuple[PyAlignedArticle, list[PyArticle]]:
        ...

    def __str__(self) -> str:
        ...

    def __repr__(self) -> str:
        ...

    def __getitem__(self, item:str | LanguageHint) -> typing.Optional[PyArticle]:
        ...

    def __contains__(self, item:str | LanguageHint) -> bool:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyAlignedArticle:
        ...


class PyAlignedArticleIter:
    def __iter__(self) -> PyAlignedArticleIter:
        ...

    def __next__(self) -> typing.Optional[PyAlignedArticle]:
        ...


class PyAlignedArticleParsedIter:
    def __iter__(self) -> PyAlignedArticleParsedIter:
        ...

    def __next__(self) -> typing.Optional[PyTokenizedAlignedArticle]:
        ...


class PyAlignedArticleProcessor:
    def __new__(cls,processors:typing.Mapping[str | LanguageHint, PyTokenizerBuilder]): ...
    def __getitem__(self, value:str | LanguageHint) -> typing.Optional[PyTokenizerBuilder]:
        ...

    def process(self, value:PyAlignedArticle) -> PyTokenizedAlignedArticle:
        ...

    def __contains__(self, language_hint:str | LanguageHint) -> bool:
        ...

    def process_string(self, language_hint:str | LanguageHint, value:str) -> typing.Optional[list[tuple[str, PyToken]]]:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyAlignedArticleProcessor:
        ...


class PyArticle:
    py_is_list: bool
    py_lang: LanguageHint
    py_categories: typing.Optional[list[int]]
    py_content: typing.Optional[str]
    def __new__(cls,language_hint,content,categories = ...,is_list = ...): ...
    def __str__(self) -> str:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyArticle:
        ...


class PyBasicBoostConfig:
    def __new__(cls,divergence,alpha = ...,target_fields = ...,invert_target_fields = ...,score_modifier_calculator = ...): ...
    ...

class PyClassifierOption:
    stop_words: typing.Optional[PyStopWords]
    separators: typing.Optional[list[str]]
    def __new__(cls,): ...
    def set_separators(self, value:typing.Optional[typing.Sequence[str]]) -> None:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyClassifierOption:
        ...


class PyContextWithMutableVariables:
    r"""
    This is an unsafe reference to a VotingMethodContext.
    If a python user saves them outside of the method, there will be a memory error.
    """
    def __getitem__(self, item:str) -> str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]:
        ...

    def __setitem__(self, key:str, value:str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]) -> None:
        ...

    def __contains__(self, item:str) -> bool:
        ...

    def get_all_values(self) -> dict[str, str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]]:
        ...


class PyDictIter:
    def __iter__(self) -> PyDictIter:
        ...

    def __next__(self) -> typing.Optional[tuple[tuple[int, str, typing.Optional[LoadedMetadataEx]], tuple[int, str, typing.Optional[LoadedMetadataEx]], DirectionMarker]]:
        ...


class PyDictMetaVector:
    r"""
    The topic vector is basically a map between [`Domain`] and [`Register`] and some score.
    Usually the score is only the count, but it can be anything.
    """
    def __new__(cls,): ...
    def __add__(self, other:PyDictMetaVector | tuple[Domain, float] | tuple[Register, float]) -> PyDictMetaVector:
        ...

    def __getitem__(self, index:Domain | Register | int) -> float:
        r"""
        Allows to get the value for a specific topic vector.
        Fails with an IndexError iff the index is outside of ht len of this vector.
        """
        ...

    def __str__(self) -> str:
        ...

    def __len__(self) -> int:
        ...

    def to_list(self) -> list[float]:
        ...


class PyDictionary:
    r"""
    A dictionary for bidirectional dictionaries.
    """
    known_dictionaries: list[str]
    translation_direction: tuple[typing.Optional[LanguageHint], typing.Optional[LanguageHint]]
    voc_a_py: PyVocabulary
    voc_b_py: PyVocabulary
    def __new__(cls,language_a:typing.Optional[str | LanguageHint], language_b:typing.Optional[str | LanguageHint]): ...
    def search(self, query,search_type = ...,threshold = ...,target_language = ...,ignores_ascii_case = ...) -> SearchResultContainerType:
        r"""
        A search function for the dictionary, requires a query or matcher as first argument.
        
        :param query: The query can be a single words, multiple words or a matcher function.
        
        :param search_type: Determines which kind of search is execute. (default: SearchType.ExactMatch)
                            See SearchType for more search types.
                            Typing is not supported when using a matcher as query.
        
        :param threshold: The threshold is needed when using any kind of distance in as search.
        
        :param target_language: The target language can either be A, B or None and determines which vocabularies are searched. (see LanguageKind)
                                None indicates to search in both vocabularies. (default: None)
        
        :param ignores_ascii_case: If the flag is set, the most searches ignore the case while comparing words. (default: false)
                                   The feature is not available for autocompletion or when using a custom matcher.
        :returns: Returns a tuple that can look like this (depending on the query):
            When no target language is set:
                - For simple query types: (list[str], list[str])
                - For complex query types: (dict[str, dict[str, set[str]]], dict[str, dict[str, set[str]]])
            When A is set as target language:
                - For simple query types: (list[str], None)
                - For complex query types: (dict[str, dict[str, set[str]]], None)
            When B is set as target language:
                - For simple query types: (None, list[str])
                - For complex query types: (None, dict[str, dict[str, set[str]]])
            The dict for a complex result is the following:
                {<query_string>: {<prefix>: {<values>}}}
            The list for simple queries basically only contains all the results in a bulk result.
        """
        ...

    def set_translation_direction(self, direction:tuple[typing.Optional[str | LanguageHint], typing.Optional[str | LanguageHint]]) -> None:
        r"""
        Allows to set the translation languages. This is usually not necessary, except you build your own.
        """
        ...

    def set_lang_a(self, value) -> None:
        ...

    def set_lang_b(self, value) -> None:
        ...

    def topic_vector_a(self, word:str) -> typing.Optional[PyDictMetaVector]:
        r"""
        Returns the topic vetor vor a specific word. Can be None if the word does not exist or
        no metadata is set.
        """
        ...

    def topic_vector_b(self, word:str) -> typing.Optional[PyDictMetaVector]:
        r"""
        Returns the topic vetor vor a specific word. Can be None if the word does not exist or
        no metadata is set.
        """
        ...

    def has_unaltered_voc(self) -> bool:
        r"""
        Returns true iff there is any content in the unaltered voc.
        """
        ...

    def meta_len(self) -> tuple[int, int]:
        r"""
        The length of the metadata
        """
        ...

    def count_unaltered_voc(self) -> tuple[int, int]:
        r"""
        Returns the number of words that know their unaltered vocabulary
        """
        ...

    def voc_a_contains(self, value:str) -> bool:
        r"""
        Returns true if voc a contains the value
        """
        ...

    def voc_b_contains(self, value:str) -> bool:
        r"""
        Returns true if voc a contains the value
        """
        ...

    def __contains__(self, value:str) -> bool:
        r"""
        Returns true if any voc contains the value
        """
        ...

    def switch_a_to_b(self) -> PyDictionary:
        ...

    def add(self, word_a:tuple[str, typing.Optional[LoadedMetadataEx]], word_b:tuple[str, typing.Optional[LoadedMetadataEx]]) -> tuple[int, int, DirectionMarker]:
        r"""
        Insert a translation from word a to b with the provided data.
        """
        ...

    def get_translation_a_to_b(self, word:str) -> typing.Optional[list[str]]:
        r"""
        Returns the translations of the word, from a to b
        """
        ...

    def get_translation_b_to_a(self, word:str) -> typing.Optional[list[str]]:
        r"""
        Returns the translations of the word, from b to a
        """
        ...

    def __repr__(self) -> str:
        ...

    def __str__(self) -> str:
        ...

    def save(self, path:str | os.PathLike | pathlib.Path) -> None:
        r"""
        Writes the dictionary to the path, the mode is chosen based on the file ending.
        """
        ...

    def save_as(self, path:str | os.PathLike | pathlib.Path, mode:typing.Literal["b", "binary", "b+c", "binary+compressed", "json", "j", "json+compressed", "j+c", "json+pretty", "j+p", "json+pretty+compressed", "j+p+c"]) -> None:
        r"""
        Writes the dictionary to the path with the chosen mode
        """
        ...

    @staticmethod
    def load(path:str | os.PathLike | pathlib.Path) -> PyDictionary:
        r"""
        Loads the dictionary from the path with the provided mode
        """
        ...

    @staticmethod
    def load_as(path:str | os.PathLike | pathlib.Path, mode:typing.Literal["b", "binary", "b+c", "binary+compressed", "json", "j", "json+compressed", "j+c", "json+pretty", "j+p", "json+pretty+compressed", "j+p+c"]) -> PyDictionary:
        r"""
        Loads the dictionary from the path with the provided mode
        """
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyDictionary:
        ...

    def __iter__(self) -> PyDictIter:
        ...

    def iter_meta_a(self) -> PyMetaIter:
        ...

    def iter_meta_b(self) -> PyMetaIter:
        ...

    def filter(self, filter_a:typing.Callable[[str, typing.Optional[LoadedMetadataEx]], bool], filter_b:typing.Callable[[str, typing.Optional[LoadedMetadataEx]], bool]) -> PyDictionary:
        r"""
        Filters a dictionary by the defined methods and returns a new instance.
        """
        ...

    def get_meta_a_of(self, word:str) -> typing.Optional[LoadedMetadataEx]:
        r"""
        Returns the meta for a specific word in a
        """
        ...

    def get_meta_b_of(self, word:str) -> typing.Optional[LoadedMetadataEx]:
        r"""
        Returns the meta for a specific word in b
        """
        ...

    def process_and_filter(self, lang_a_proc:typing.Callable[[str], typing.Optional[tuple[str, typing.Optional[str]]]], lang_b_proc:typing.Callable[[str], typing.Optional[tuple[str, typing.Optional[str]]]]) -> PyDictionary:
        ...

    def process_with_tokenizer(self, processor:PyAlignedArticleProcessor) -> PyDictionary:
        r"""
        Creates a new dictionary where the processor was applied.
        Requires that both languages (a+b) are properly set.
        """
        ...

    def create_html_view_in(self, path,crc32 = ...,thread_limit = ...) -> None:
        r"""
        Creates an html view of the vocabulary in this folder.
        If the thread limit is not set, the global thread pool will try to use the optimal number
        of threads for this CPU for complete usage.
        If the thread_limit is set to zero the number of threads will be chosen automatically.
        If the thread limit is grater than 0 this function will spawn n threads to create the necessary files.
        
        If crc32 is set to false, it does noch check the file contents before overriding it.
        Otherwise it it uses crc32 before overriding a file, to check if it is similar or not. (default)
        The algorithm used is the fast, SIMD-accelerated CRC32 (IEEE) checksum computation.
        
        The probability for a collision can be found here: https://preshing.com/20110504/hash-collision-probabilities/
        """
        ...

    def len(self) -> Len:
        r"""
        Returns a len object, containing the counts of the single parts
        """
        ...

    def get_fields_with_content(self) -> list[MetaField]:
        r"""
        Returns a list of fields that hold data.
        """
        ...

    def drop_metadata_field(self, field:MetaField) -> bool:
        r"""
        Drops the metadata from a field.
        Returns true iff data was lost.
        """
        ...

    def drop_all_except(self, * py_args) -> list[tuple[MetaField, bool]]:
        r"""
        Drops all fields except the one in the args.
        Returns a list of dropped fields and if data was dropped.
        """
        ...

    def dictionary_meta_counts(self) -> DictMetaCounts:
        ...


class PyHorizontalBoostConfig:
    def __new__(cls,divergence,mean_method = ...,normalize_mode = ...,alpha = ...,linear_transformed = ...,transform = ...,factor = ...,only_positive_boost = ...): ...
    ...

class PyMetaIter:
    def __iter__(self) -> PyMetaIter:
        ...

    def __next__(self) -> typing.Optional[tuple[int, str, typing.Optional[LoadedMetadataEx]]]:
        ...


class PyNGramBoostConfig:
    def __new__(cls,boost_lang_a = ...,boost_lang_b = ...): ...
    ...

class PyNGramLanguageBoost:
    def __new__(cls,idf = ...,boosting = ...,norm = ...,factor = ...,fallback_language = ...,only_positive_boost = ...): ...
    ...

class PyNGramStatistics:
    @staticmethod
    def load(path:str | os.PathLike | pathlib.Path) -> PyNGramStatistics:
        ...


class PyNormalizerOption:
    create_char_map: bool
    classifier: PyClassifierOption
    lossy: bool
    def __new__(cls,): ...
    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyNormalizerOption:
        ...


class PyParsedAlignedArticleIter:
    def __iter__(self) -> PyParsedAlignedArticleIter:
        ...

    def __next__(self) -> typing.Optional[PyTokenizedAlignedArticle]:
        ...


class PySegmenterOption:
    aho: typing.Optional[PyAhoCorasick]
    allow_list: typing.Optional[dict[PyScript, list[PyLanguage]]]
    def __new__(cls,): ...
    def set_allow_list(self, allow_list:typing.Optional[typing.Mapping[PyScript, typing.Sequence[PyLanguage]]]) -> None:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PySegmenterOption:
        ...


class PyStopWords:
    def __new__(cls,words:set[str] | list[str]): ...
    def __contains__(self, value:str) -> bool:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyStopWords:
        ...


class PyToken:
    kind: PyTokenKind
    lemma: str
    char_start: int
    char_end: int
    byte_start: int
    byte_end: int
    char_map: typing.Optional[list[tuple[int, int]]]
    script: PyScript
    language: typing.Optional[PyLanguage]
    def byte_len(self) -> int:
        ...

    def __len__(self) -> int:
        ...

    def __str__(self) -> str:
        ...

    def __repr__(self) -> str:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyToken:
        ...


class PyTokenizedAlignedArticle:
    article_id: int
    language_hints: list[LanguageHint]
    def __new__(cls,article_id:int, articles:typing.Mapping[str | LanguageHint, PyArticle | list[tuple[str, PyToken]]]): ...
    @staticmethod
    def create(article_id:int, articles:dict[str | LanguageHint, PyArticle | list[tuple[str, PyToken]]] | list[PyArticle | list[tuple[str, PyToken]]]) -> PyTokenizedAlignedArticle | tuple[PyTokenizedAlignedArticle, list[PyArticle | list[tuple[str, PyToken]]]]:
        ...

    def __str__(self) -> str:
        ...

    def __repr__(self) -> str:
        ...

    def __getitem__(self, item:str | LanguageHint) -> typing.Optional[PyArticle | list[tuple[str, PyToken]]]:
        ...

    def __contains__(self, item:str | LanguageHint) -> bool:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyTokenizedAlignedArticle:
        ...


class PyTokenizerBuilder:
    def __new__(cls,): ...
    def stemmer(self, stemmer,smart = ...) -> PyTokenizerBuilder:
        ...

    def phrase_vocabulary(self, vocabulary) -> PyTokenizerBuilder:
        ...

    def stop_words(self, stop_words) -> PyTokenizerBuilder:
        ...

    def separators(self, separators) -> PyTokenizerBuilder:
        ...

    def words_dict(self, words) -> PyTokenizerBuilder:
        ...

    def create_char_map(self, create_char_map) -> PyTokenizerBuilder:
        ...

    def lossy_normalization(self, lossy) -> PyTokenizerBuilder:
        ...

    def unicode_segmentation(self, unicode) -> PyTokenizerBuilder:
        ...

    def allow_list(self, allow_list) -> PyTokenizerBuilder:
        ...

    def create_stopword_filter(self) -> typing.Optional[PyStopWords]:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyTokenizerBuilder:
        ...

    def __repr__(self) -> str:
        ...


class PyTopicModel:
    py_k: int
    def __new__(cls,topics:typing.Sequence[typing.Sequence[float]], vocabulary:PyVocabulary, used_vocab_frequency:typing.Sequence[int], doc_topic_distributions:typing.Sequence[typing.Sequence[float]], document_lengths:typing.Sequence[int]): ...
    def get_topic(self, topic_id:int) -> typing.Optional[list[float]]:
        ...

    def show_top(self, n = ...) -> None:
        ...

    def __repr__(self) -> str:
        ...

    def __str__(self) -> str:
        ...

    def get_doc_probability(self, doc,alpha,gamma_threshold,minimum_probability = ...,minimum_phi_value = ...,per_word_topics = ...) -> tuple[list[tuple[int, float]], typing.Optional[list[tuple[int, list[int]]]], typing.Optional[list[tuple[int, list[tuple[int, float]]]]]]:
        ...

    def vocabulary(self) -> PyVocabulary:
        ...

    def get_words_of_topic_sorted(self, topic_id:int) -> typing.Optional[list[tuple[str, float]]]:
        ...

    def get_topic_as_words(self, topic_id:int) -> typing.Optional[list[tuple[int, str, float]]]:
        ...

    def translate_by_provided_word_lists(self, language_hint:str | LanguageHint, word_lists:list[str] | list[list[str]]) -> PyTopicModel:
        ...

    def save_json(self, path:str | os.PathLike | pathlib.Path) -> None:
        ...

    @staticmethod
    def load_json(path:str | os.PathLike | pathlib.Path) -> PyTopicModel:
        ...

    def save_binary(self, path:str | os.PathLike | pathlib.Path) -> None:
        ...

    @staticmethod
    def load_binary(path:str | os.PathLike | pathlib.Path) -> PyTopicModel:
        ...

    def normalize(self) -> PyTopicModel:
        ...

    @staticmethod
    def builder(language = ...) -> PyTopicModelBuilder:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyTopicModel:
        ...


class PyTopicModelBuilder:
    def __new__(cls,language = ...): ...
    def set_frequency(self, word:str, frequency:int) -> None:
        ...

    def add_word(self, topic_id,word,probability,frequency = ...) -> None:
        ...

    def set_doc_topic_distributions(self, doc_topic_distributions = ...) -> None:
        ...

    def set_document_lengths(self, document_lengths = ...) -> None:
        ...

    def build(self, unset_words_become_smallest = ...,normalize = ...) -> PyTopicModel:
        ...


class PyTranslationConfig:
    def __new__(cls,epsilon = ...,threshold = ...,keep_original_word = ...,top_candidate_limit = ...,vertical_config = ...,horizontal_config = ...,ngram_config = ...): ...
    ...

class PyVariableProvider:
    def __new__(cls,): ...
    def add_global(self, key:str, value:str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]) -> typing.Optional[str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]]:
        ...

    def add_for_topic(self, topic_id:int, key:str, value:str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]) -> None:
        ...

    def add_for_word_a(self, word:str, key:str, value:str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]) -> None:
        ...

    def add_for_word_b(self, word:str, key:str, value:str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]) -> None:
        ...

    def add_for_word_in_topic_a(self, topic_id:int, word:str, key:str, value:str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]) -> None:
        ...

    def add_for_word_in_topic_b(self, topic_id:int, word:str, key:str, value:str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]) -> None:
        ...


class PyVerticalBoostConfig:
    def __new__(cls,divergence,transformer = ...,factor = ...,only_positive_boost = ...): ...
    ...

class PyVocIter:
    def __iter__(self) -> PyVocIter:
        ...

    def __next__(self) -> str:
        r"""
        May raise an assertion error when the vocabulary changes while iterating.
        """
        ...


class PyVocabulary:
    language_hint: typing.Optional[LanguageHint]
    def __new__(cls,language = ...,size = ...): ...
    def set_language(self, value:typing.Optional[str | LanguageHint]) -> None:
        ...

    def __repr__(self) -> str:
        ...

    def __str__(self) -> str:
        ...

    def __len__(self) -> int:
        ...

    def __contains__(self, value:str) -> bool:
        ...

    def __iter__(self) -> PyVocIter:
        ...

    def add(self, word:str) -> int:
        ...

    def word_to_id(self, word:str) -> typing.Optional[int]:
        ...

    def id_to_word(self, id:int) -> typing.Optional[str]:
        ...

    def save(self, path:str | os.PathLike | pathlib.Path) -> int:
        r"""
        Save the vocabulary in a standardisized way
        """
        ...

    @staticmethod
    def load(path:str | os.PathLike | pathlib.Path) -> PyVocabulary:
        r"""
        Load the vocabulary from a file
        """
        ...

    def to_json(self) -> str:
        r"""
        Serializes this to a json
        """
        ...

    @staticmethod
    def from_json(s:str) -> PyVocabulary:
        r"""
        Deserializes a json to a vocabulary.
        """
        ...


class PyVoting:
    @staticmethod
    def parse(value,registry = ...) -> PyVoting:
        ...

    def __call__(self, global_context:PyContextWithMutableVariables, voters:typing.Sequence[PyContextWithMutableVariables]) -> tuple[str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]], list[PyContextWithMutableVariables]]:
        ...


class PyVotingRegistry:
    def __new__(cls,): ...
    def get_registered(self, name:str) -> typing.Optional[PyVoting]:
        ...

    def register_at(self, name:str, voting:str) -> None:
        ...

    def register(self, voting:str) -> None:
        ...


class StoreOptions:
    r"""
    The options for storing.
    """
    deflate_temp_files: bool
    delete_temp_files_immediately: bool
    compress_result: bool
    show_progress_after: typing.Optional[int]
    temp_folder: typing.Optional[str]
    def __new__(cls,deflate_temp_files = ...,delete_temp_files_immediately = ...,compress_result = ...,temp_folder = ...,show_progress_after = ...): ...
    def show_progress_after(self, show_progress_after:int) -> None:
        ...

    def temp_folder(self, temp_folder:typing.Optional[str | os.PathLike | pathlib.Path]) -> None:
        ...


class TokenCountFilter:
    min: typing.Optional[int]
    max: typing.Optional[int]
    def __new__(cls,min = ...,max = ...): ...
    def set_min(self, min:typing.Optional[int]) -> None:
        ...

    def set_max(self, max:typing.Optional[int]) -> None:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> TokenCountFilter:
        ...

    def __contains__(self, value:int) -> bool:
        ...


class BoostMethod(Enum):
    r"""
    Setting if to keep the original word from language A
    """
    Linear = auto()
    Sum = auto()
    Mult = auto()
    MultPow = auto()
    Pipe = auto()

class BoostNorm(Enum):
    Off = auto()
    Linear = auto()
    Normalized = auto()

class BuildInVoting(Enum):
    r"""
    All possible buildin votings
    """
    OriginalScore = auto()
    Voters = auto()
    CombSum = auto()
    GCombSum = auto()
    CombSumTop = auto()
    CombSumPow2 = auto()
    CombMax = auto()
    RR = auto()
    RRPow2 = auto()
    CombSumRR = auto()
    CombSumRRPow2 = auto()
    CombSumPow2RR = auto()
    CombSumPow2RRPow2 = auto()
    ExpCombMnz = auto()
    WCombSum = auto()
    WCombSumG = auto()
    WGCombSum = auto()
    PCombSum = auto()

class DictionaryLanguage(Enum):
    r"""
    The recognized
    """
    English = auto()
    German = auto()
    Italian = auto()
    French = auto()
    Latin = auto()

class DirectionMarker(Enum):
    r"""
    The direction of the language
    """
    AToB = auto()
    BToA = auto()
    Invariant = auto()

class Domain(Enum):
    Acad = auto()
    Acc = auto()
    Admin = auto()
    Agr = auto()
    Anat = auto()
    Archaeo = auto()
    Archi = auto()
    Armour = auto()
    Art = auto()
    Astrol = auto()
    Astron = auto()
    Astronau = auto()
    Audio = auto()
    Automot = auto()
    Aviat = auto()
    Bibl = auto()
    Bike = auto()
    Biochem = auto()
    Biol = auto()
    Biotech = auto()
    Bot = auto()
    Brew = auto()
    Chem = auto()
    Climbing = auto()
    Cloth = auto()
    Comics = auto()
    Comm = auto()
    Comp = auto()
    Constr = auto()
    Cook = auto()
    Cosmet = auto()
    Curr = auto()
    Dance = auto()
    Dent = auto()
    Drugs = auto()
    Ecol = auto()
    Econ = auto()
    Educ = auto()
    Electr = auto()
    Engin = auto()
    Entom = auto()
    Equest = auto()
    Esot = auto()
    Ethn = auto()
    Eu = auto()
    F = auto()
    Film = auto()
    Fin = auto()
    FireResc = auto()
    Fish = auto()
    FoodInd = auto()
    For = auto()
    Furn = auto()
    Games = auto()
    Gastr = auto()
    Geogr = auto()
    Geol = auto()
    Herald = auto()
    Hist = auto()
    Hort = auto()
    Hunting = auto()
    Hydro = auto()
    Idiom = auto()
    Ind = auto()
    Insur = auto()
    Internet = auto()
    Jobs = auto()
    Journ = auto()
    Law = auto()
    Libr = auto()
    Ling = auto()
    Lit = auto()
    Mach = auto()
    Market = auto()
    Material = auto()
    Math = auto()
    Med = auto()
    MedTech = auto()
    Meteo = auto()
    Mil = auto()
    Mineral = auto()
    Mining = auto()
    Mus = auto()
    Mycol = auto()
    Myth = auto()
    Name = auto()
    Naut = auto()
    Neol = auto()
    Nucl = auto()
    Oenol = auto()
    Optics = auto()
    Orn = auto()
    Pharm = auto()
    Philat = auto()
    Philos = auto()
    Phonet = auto()
    Photo = auto()
    Phys = auto()
    Pol = auto()
    Print = auto()
    Proverb = auto()
    Psych = auto()
    Publ = auto()
    Qm = auto()
    Quote = auto()
    RadioTv = auto()
    Rail = auto()
    RealEst = auto()
    Relig = auto()
    Rhet = auto()
    School = auto()
    Sociol = auto()
    Spec = auto()
    Sports = auto()
    Stat = auto()
    Stocks = auto()
    Stud = auto()
    T = auto()
    Tech = auto()
    Telecom = auto()
    Textil = auto()
    Theatre = auto()
    Tools = auto()
    Toys = auto()
    TrVocab = auto()
    Traffic = auto()
    Transp = auto()
    Travel = auto()
    Unit = auto()
    Urban = auto()
    Uwh = auto()
    VetMed = auto()
    Watches = auto()
    Weapons = auto()
    Zool = auto()
    Child = auto()
    Youth = auto()
    Science = auto()
    Poetry = auto()
    Currency = auto()
    Phila = auto()
    Commun = auto()
    Media = auto()
    Tour = auto()
    Alchemy = auto()
    Anime = auto()
    Bever = auto()
    Sex = auto()
    Palaeo = auto()
    Metal = auto()
    Masonry = auto()
    Colour = auto()
    Mechanics = auto()
    Money = auto()
    NatSci = auto()
    PseudoSci = auto()
    Humanities = auto()

class FDivergence(Enum):
    Renyi = auto()
    Total = auto()
    ChiAlpha = auto()
    KL = auto()
    KLReversed = auto()
    JensenShannon = auto()
    Jeffrey = auto()
    Bhattacharyya = auto()
    Hellinger = auto()
    PearsonChiSquare = auto()
    NeymanChiSquare = auto()

class GrammaticalGender(Enum):
    Feminine = auto()
    Masculine = auto()
    Neutral = auto()
    NotFeminine = auto()

class GrammaticalNumber(Enum):
    Singular = auto()
    Plural = auto()
    NoSingular = auto()
    NoPlural = auto()
    UsuallyPlural = auto()
    SingularOnly = auto()
    PluralOnly = auto()

class Idf(Enum):
    r"""
    Default IDF Algorithms
    From https://en.wikipedia.org/wiki/Tf%E2%80%93idf
    """
    Unary = auto()
    InverseDocumentFrequency = auto()
    InverseDocumentFrequencySmooth = auto()
    InverseDocumentFrequencyMax = auto()
    ProbabilisticInverseDocumentFrequency = auto()

class KeepOriginalWord(Enum):
    r"""
    Setting if to keep the original word from language A
    """
    Always = auto()
    IfNoTranslation = auto()
    Never = auto()

class LanguageMarker(Enum):
    r"""
    The language
    """
    A = auto()
    B = auto()

class MeanMethod(Enum):
    r"""
    Setting if to keep the original word from language A
    """
    ArithmeticMean = auto()
    LinearWeightedArithmeticMean = auto()
    HarmonicMean = auto()
    LinearWeightedHarmonicMean = auto()
    GeometricMean = auto()
    LinearWeightedGeometricMean = auto()
    Median = auto()

class MetaField(Enum):
    Languages = auto()
    Domains = auto()
    Registers = auto()
    Genders = auto()
    Pos = auto()
    PosTag = auto()
    Regions = auto()
    Numbers = auto()
    InternalIds = auto()
    Inflected = auto()
    Abbreviations = auto()
    UnalteredVocabulary = auto()
    LookAt = auto()
    Ids = auto()
    OutgoingIds = auto()
    OriginalEntry = auto()
    ContextualInformations = auto()
    Unclassified = auto()
    Synonyms = auto()

class NormalizeMode(Enum):
    Max = auto()
    Sum = auto()

class PartOfSpeech(Enum):
    Noun = auto()
    Verb = auto()
    Adv = auto()
    Pron = auto()
    Conj = auto()
    Prep = auto()
    Det = auto()
    Intj = auto()
    Num = auto()
    Article = auto()
    Name = auto()
    Prefix = auto()
    Suffix = auto()
    Character = auto()
    Particle = auto()
    Other = auto()
    Abbrev = auto()
    Adj = auto()
    AdjNoun = auto()
    AdjVerb = auto()
    Adnominal = auto()
    AdvPhrase = auto()
    Affix = auto()
    Ambiposition = auto()
    Circumfix = auto()
    Circumpos = auto()
    Classifier = auto()
    Clause = auto()
    CombiningForm = auto()
    Contraction = auto()
    Converb = auto()
    Counter = auto()
    Infix = auto()
    Interfix = auto()
    Phrase = auto()
    Postp = auto()
    PrepPhrase = auto()
    Preverb = auto()
    Proverb = auto()
    Punct = auto()
    Romanization = auto()
    Root = auto()
    Syllable = auto()
    Symbol = auto()

class PartOfSpeechTag(Enum):
    Morpheme = auto()
    Participle = auto()
    Present = auto()
    Transitive = auto()
    Ideophone = auto()
    Perfect = auto()
    Verbal = auto()
    Gerund = auto()
    Infinitive = auto()
    Interrogative = auto()
    Intransitive = auto()
    Past = auto()
    Person = auto()
    Prepositional = auto()
    Relative = auto()
    Hanja = auto()
    Abbreviation = auto()
    Clitic = auto()
    Comparative = auto()
    Dependent = auto()
    Diacritic = auto()
    Han = auto()
    Hanzi = auto()
    Idiomatic = auto()
    Indefinite = auto()
    Kanji = auto()
    Letter = auto()
    Ligature = auto()
    Number = auto()
    Ordinal = auto()
    Possessive = auto()
    Predicative = auto()
    Punctuation = auto()

class PyAhoCorasickKind(Enum):
    NoncontiguousNFA = auto()
    ContiguousNFA = auto()
    DFA = auto()

class PyLanguage(Enum):
    Epo = auto()
    Eng = auto()
    Rus = auto()
    Cmn = auto()
    Spa = auto()
    Por = auto()
    Ita = auto()
    Ben = auto()
    Fra = auto()
    Deu = auto()
    Ukr = auto()
    Kat = auto()
    Ara = auto()
    Hin = auto()
    Jpn = auto()
    Heb = auto()
    Yid = auto()
    Pol = auto()
    Amh = auto()
    Jav = auto()
    Kor = auto()
    Nob = auto()
    Dan = auto()
    Swe = auto()
    Fin = auto()
    Tur = auto()
    Nld = auto()
    Hun = auto()
    Ces = auto()
    Ell = auto()
    Bul = auto()
    Bel = auto()
    Mar = auto()
    Kan = auto()
    Ron = auto()
    Slv = auto()
    Hrv = auto()
    Srp = auto()
    Mkd = auto()
    Lit = auto()
    Lav = auto()
    Est = auto()
    Tam = auto()
    Vie = auto()
    Urd = auto()
    Tha = auto()
    Guj = auto()
    Uzb = auto()
    Pan = auto()
    Aze = auto()
    Ind = auto()
    Tel = auto()
    Pes = auto()
    Mal = auto()
    Ori = auto()
    Mya = auto()
    Nep = auto()
    Sin = auto()
    Khm = auto()
    Tuk = auto()
    Aka = auto()
    Zul = auto()
    Sna = auto()
    Afr = auto()
    Lat = auto()
    Slk = auto()
    Cat = auto()
    Tgl = auto()
    Hye = auto()
    Other = auto()

class PyMatchKind(Enum):
    Standard = auto()
    LeftmostFirst = auto()
    LeftmostLongest = auto()

class PyScript(Enum):
    Arabic = auto()
    Armenian = auto()
    Bengali = auto()
    Cyrillic = auto()
    Devanagari = auto()
    Ethiopic = auto()
    Georgian = auto()
    Greek = auto()
    Gujarati = auto()
    Gurmukhi = auto()
    Hangul = auto()
    Hebrew = auto()
    Kannada = auto()
    Khmer = auto()
    Latin = auto()
    Malayalam = auto()
    Myanmar = auto()
    Oriya = auto()
    Sinhala = auto()
    Tamil = auto()
    Telugu = auto()
    Thai = auto()
    Cj = auto()
    Other = auto()

class PyStartKind(Enum):
    Both = auto()
    Unanchored = auto()
    Anchored = auto()

class PyStemmingAlgorithm(Enum):
    Arabic = auto()
    Danish = auto()
    Dutch = auto()
    English = auto()
    Finnish = auto()
    French = auto()
    German = auto()
    Greek = auto()
    Hungarian = auto()
    Italian = auto()
    Norwegian = auto()
    Portuguese = auto()
    Romanian = auto()
    Russian = auto()
    Spanish = auto()
    Swedish = auto()
    Tamil = auto()
    Turkish = auto()

class PyTokenKind(Enum):
    Word = auto()
    StopWord = auto()
    SeparatorHard = auto()
    SeparatorSoft = auto()
    Unknown = auto()

class Region(Enum):
    BritishEnglish = auto()
    AmericanEnglish = auto()
    AustralianEnglish = auto()
    NewZealandEnglish = auto()
    CanadianEnglish = auto()
    IrishEnglish = auto()
    IndianEnglish = auto()
    SouthAfricanEnglish = auto()
    ScottishEnglish = auto()
    AustrianGerman = auto()
    SouthGerman = auto()
    NorthGerman = auto()
    EastGerman = auto()
    SwissGerman = auto()
    Regional = auto()
    MiddleWestGerman = auto()
    SouthWestGerman = auto()
    NorthWestGerman = auto()
    BadenWuerttembergGerman = auto()
    MiddleEastGerman = auto()
    SouthEastGerman = auto()
    NorthEastGerman = auto()
    MiddleGerman = auto()
    BavarianGerman = auto()
    NorthernIrish = auto()
    UpperGerman = auto()
    EastAustrianGerman = auto()
    BerlinGerman = auto()
    SwabianGerman = auto()
    WestAustrianGerman = auto()
    ViennaGerman = auto()
    TyrolGerman = auto()
    NorthEnglish = auto()
    DDRGerman = auto()
    PfalzGerman = auto()
    SouthTyrolGerman = auto()
    EastMiddleGerman = auto()
    SouthEastAsianEnglish = auto()
    HesseGerman = auto()
    LuxenbourgGerman = auto()
    WelchEnglish = auto()
    RhinelandPalatinateGerman = auto()
    SaxonyGerman = auto()
    WestGerman = auto()
    LiechtensteinGerman = auto()
    WestphaliaGerman = auto()
    SouthEastAustrianGerman = auto()
    NorthEastAustrianGerman = auto()
    NorthWestAustrianGerman = auto()
    SouthWestAustrianGerman = auto()
    WestSwissGerman = auto()
    NorthIrishEnglish = auto()
    MiddleAustrianGerman = auto()
    FranconianGerman = auto()
    EastSwissGerman = auto()
    Germanic = auto()

class Register(Enum):
    r"""
    In sociolinguistics, a register is a variety of language used for a particular purpose or particular communicative situation
    """
    Humor = auto()
    Vulg = auto()
    Techn = auto()
    Coll = auto()
    Geh = auto()
    Slang = auto()
    Iron = auto()
    Formal = auto()
    Euphem = auto()
    Literary = auto()
    Dialect = auto()
    Archaic = auto()
    Rare = auto()
    Pejorativ = auto()
    Figurative = auto()
    AlsoFigurative = auto()
    SpellingVariant = auto()
    Admin = auto()
    Transfer = auto()
    NetJargon = auto()
    Informal = auto()
    QuantityInformation = auto()
    IATEPreferred = auto()
    Miss = auto()
    ComGend = auto()

class ScoreModifierCalculator(Enum):
    Max = auto()
    WeightedSum = auto()

class SearchType(Enum):
    r"""
    The modes supported by the search.
    - ExactMatch (aliases: "e", "exact")
        Matches the word or words provided in an exact manner.
    
    - Autocomplete (aliases: "a", "auto")
        Uses a prefix trie to search for words with a common prefix.
    
    - Contains (aliases: "c", "contains")
        Returns all words that contain the same string
    
    - StartsWith (aliases: "start", "starts_with")
        Returns all words that start with the same string.
    
    - EndsWith (aliases: "end", "ends_with")
        Returns all words that end with the same string
    
    - Regex (aliases: "r", "reg", "regex")
        Matches one or multiple regex expressions, returns all words that match.
    
    - Hamming (aliases: "h", "ham", "hamming")
        Returns everything that is <= threshold for a hamming distance between two words.
    
    - Levensthein (aliases: "l", "lev", "levensthein")
        Returns everything that is <= threshold for a levensthein distance between two words.
    
    - OsaDistance (aliases: "o", "osa", "osa_dist", "osa_distance")
        Returns everything that is <= threshold for a osa distance between two words.
        Like Levenshtein but allows for adjacent transpositions. Each substring can only be edited once.
    
    - NormalizedLevensthein (aliases: "l+n", "lev_norm", "lev_normalized")
        Returns everything that is >= threshold for a normalized levensthein distance between two words.
        The normalized value is between 0.0 and 1.0, where 1.0 means that strings are the same.
    
    - DamerauLevensthein (aliases: "dl", "dam_lev", "damerau_levensthein")
        Returns everything that is <= threshold for a damerau levensthein distance between two words.
    
    - NormalizedDamerauLevensthein (aliases: "dl+n", "dam_lev_norm", "damerau_levensthein_norm", "damerau_levensthein_normalized"):
        Returns everything that is >= threshold for a normalized damerau levensthein distance between two words.
        The normalized value is between 0.0 and 1.0, where 1.0 means that strings are the same.
    
    - Jaro (aliases: "j", "jaro")
        Returns everything that is >= threshold for a jaro distance between two words.
        The returned value is between 0.0 and 1.0 (higher value means more similar).
    
    - JaroWinkler (aliases: "jw", "jaro_winkler")
        Returns everything that is >= threshold for a jaro winkler distance between two words.
        The returned value is between 0.0 and 1.0 (higher value means more similar).
        Gives a boost to strings where a common prefix exists.
    
    - SorensenDice (aliases: "s", "soren", "sorensen_dice")
        Returns everything that is >= threshold for a sorensen dice distance between two words.
        The returned value is between 0.0 and 1.0 (higher value means more similar), where 1.0 means that strings are the same.
        Calculates a Sørensen-Dice similarity distance using bigrams. See https:// en. wikipedia. org/ wiki/ S%C3%B8rensen%E2%80%93Dice_coefficient
    
    - CommonPrefix (aliases: "cp", "com_pre", "common_prefix")
        Returns every word that has a common prefix (the provided string/strings).
    """
    ExactMatch = auto()
    Autocomplete = auto()
    Contains = auto()
    StartsWith = auto()
    EndsWith = auto()
    Regex = auto()
    Hamming = auto()
    Levensthein = auto()
    OsaDistance = auto()
    NormalizedLevensthein = auto()
    DamerauLevensthein = auto()
    NormalizedDamerauLevensthein = auto()
    Jaro = auto()
    JaroWinkler = auto()
    SorensenDice = auto()
    CommonPrefix = auto()

def create_topic_model_specific_dictionary(dictionary:PyDictionary,vocabulary:PyVocabulary) -> PyDictionary:
    ...

def generate_word_counts(inp_root:str | os.PathLike | pathlib.Path,out_root:str | os.PathLike | pathlib.Path,proc:PyAlignedArticleProcessor,v1:PyDictionary,ngrams:typing.Sequence[NGramDefinition]) -> list[tuple[str, PyNGramStatistics]]:
    ...

def load_ratings(path:str | os.PathLike | pathlib.Path) -> list[tuple[int, list[tuple[int, float]]]]:
    r"""
    Loads some ratings
    """
    ...

def read_aligned_articles(path,with_pickle = ...) -> PyAlignedArticleIter:
    ...

def read_aligned_parsed_articles(path,with_pickle = ...) -> PyAlignedArticleParsedIter:
    ...

def read_and_parse_aligned_articles(path,processor,with_pickle = ...) -> PyParsedAlignedArticleIter:
    ...

def read_and_parse_aligned_articles_into(path_in,path_out,processor,filter = ...,store_options = ...,with_pickle = ...) -> int:
    ...

def save_ratings(path:str | os.PathLike | pathlib.Path,ratings:typing.Sequence[tuple[int, typing.Sequence[tuple[int, float]]]]) -> None:
    r"""
    Saves some ratings
    """
    ...

def translate_topic_model(topic_model,dictionary,voting,config,provider = ...,voting_registry = ...,ngram_statistics = ...) -> PyTopicModel:
    ...

