# This file is automatically generated by pyo3_stub_gen
# ruff: noqa: E501, F401

import os
import pathlib
import typing
from enum import Enum, auto


# Autogenerated types for recursions etc.
type PyExprValueSingle = str | float | int | bool | None | list[PyExprValueSingle]
type ResolvedValueType = ResolvedValueTypeOut
type ResolvedValueTypeIn = DictionaryLanguage | Domain | Register | GrammaticalGender | PartOfSpeech | Region | GrammaticalNumber | PartOfSpeechTag | str | int
type ResolvedValueTypeOut = DictionaryLanguage | Domain | Register | GrammaticalGender | PartOfSpeech | Region | GrammaticalNumber | PartOfSpeechTag | str | int


CANDIDATE_ID: str
COUNT_OF_VOTERS: str
EPSILON: str
HAS_TRANSLATION: str
IMPORTANCE: str
IS_ORIGIN_WORD: str
NUMBER_OF_VOTERS: str
RANK: str
REAL_RECIPROCAL_RANK: str
RECIPROCAL_RANK: str
SCORE: str
SCORE_CANDIDATE: str
TOPIC_AVG_PROBABILITY: str
TOPIC_ID: str
TOPIC_MAX_PROBABILITY: str
TOPIC_MIN_PROBABILITY: str
TOPIC_SUM_PROBABILITY: str
VOCABULARY_SIZE_A: str
VOCABULARY_SIZE_B: str
VOTER_ID: str
class DictionaryLanguageDirection:
    lang_a: DictionaryLanguage
    lang_b: DictionaryLanguage
    def __new__(cls,lang_a:DictionaryLanguage, lang_b:DictionaryLanguage): ...
    def __contains__(self, other:DictionaryLanguage) -> bool:
        r"""
        Returns true if this contains [other] as any position
        """
        ...

    def __getitem__(self, language_kind:LanguageKind) -> DictionaryLanguage:
        ...

    def invert(self) -> DictionaryLanguageDirection:
        r"""
        Returns an inverted variant
        """
        ...

    def __invert__(self) -> DictionaryLanguageDirection:
        r"""
        Returns an inverted variant
        """
        ...

    def __neg__(self) -> DictionaryLanguageDirection:
        r"""
        Returns an inverted variant
        """
        ...

    def is_direction_in(self, lang_a:DictionaryLanguage, lang_b:DictionaryLanguage) -> bool:
        r"""
        Returns true if this points from [lang_a] to [lang_b]
        """
        ...


class DomainModel:
    def __new__(cls,capacity = ...): ...
    def __str__(self) -> str:
        ...

    def to_list(self) -> list[Entry]:
        ...


class Entry:
    def __new__(cls,): ...
    def __add__(self, other:Entry | tuple[Domain, float] | tuple[Register, float]) -> Entry:
        ...

    def __str__(self) -> str:
        ...

    def to_list(self) -> list[float]:
        ...


class LanguageHint:
    r"""
    A hint for the language used.
    """
    def __new__(cls,language:str): ...
    def __eq__(self, other:LanguageHint) -> bool:
        ...

    def __hash__(self) -> int:
        ...

    def __repr__(self) -> str:
        ...

    def __str__(self) -> str:
        ...


class Len:
    voc_a: int
    voc_b: int
    map_a_to_b: int
    map_b_to_a: int
    def __str__(self) -> str:
        ...

    def diff(self, other:Len) -> Len:
        ...


class LoadedMetadataEx:
    def py_new(self, values:dict[MetaField, ResolvedValueTypeOut | list[ResolvedValueTypeOut]] | dict[MetaField, dict[str, ResolvedValueTypeOut | list[ResolvedValueTypeOut]]] | dict[MetaField, tuple[typing.Optional[ResolvedValueTypeOut | list[ResolvedValueTypeOut]], typing.Optional[dict[str, ResolvedValueTypeOut | list[ResolvedValueTypeOut]]]]] | tuple[typing.Optional[dict[MetaField, ResolvedValueTypeOut | list[ResolvedValueTypeOut]]], typing.Optional[dict[MetaField, dict[str, ResolvedValueTypeOut | list[ResolvedValueTypeOut]]]]]) -> LoadedMetadataEx:
        ...

    def domain_vector(self) -> Entry:
        r"""
        Returns a domain vector.
        """
        ...

    def languages_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def domains_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def registers_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def genders_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def pos_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def pos_tag_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def regions_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def numbers_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def internal_ids_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def inflected_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def abbreviations_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def unaltered_vocabulary_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def look_at_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def ids_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def outgoing_ids_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def original_entry_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def contextual_informations_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def unclassified_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def synonyms_py(self) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the whole field
        """
        ...

    def get_languages_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_domains_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_registers_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_genders_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_pos_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_pos_tag_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_regions_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_numbers_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_internal_ids_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_inflected_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_abbreviations_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_unaltered_vocabulary_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_look_at_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_ids_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_outgoing_ids_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_original_entry_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_contextual_informations_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_unclassified_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_synonyms_single(self, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for this specific field. If a dictionary name is provided, it returns the value of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def __str__(self) -> str:
        ...

    def get_single_field(self, field:MetaField, dictionary:typing.Optional[str]) -> typing.Optional[set[ResolvedValueTypeOut]]:
        r"""
        Retrieves the value for a specific field. If a dictionary name is provided, it returns the values of this specific dictionary.
        Otherwise None returns the general information.
        """
        ...

    def get_field(self, field:MetaField) -> tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]:
        r"""
        Get the metadata of this specific field.
        """
        ...

    def as_dict(self) -> dict[MetaField, tuple[typing.Optional[set[ResolvedValueTypeOut]], typing.Optional[dict[str, set[ResolvedValueTypeOut]]]]]:
        r"""
        Returns the metadata as dict.
        """
        ...


class PyAhoCorasick:
    r"""
    An automaton for searching multiple strings in linear time.
    This is only used to hold the results.
    """
    @staticmethod
    def builder() -> PyAhoCorasickBuilder:
        ...


class PyAhoCorasickBuilder:
    def __new__(cls,): ...
    def build(self, patterns:typing.Sequence[str]) -> PyAhoCorasick:
        r"""
        Build an Aho-Corasick automaton using the configuration set on this
        builder.
        
        A builder may be reused to create more automatons.
        
        # Examples
        
        Basic usage:
        
        ```
        use aho_corasick::{AhoCorasickBuilder, PatternID};
        
        let patterns = &["foo", "bar", "baz"];
        let ac = AhoCorasickBuilder::new().build(patterns).unwrap();
        assert_eq!(
            Some(PatternID::must(1)),
            ac.find("xxx bar xxx").map(|m| m.pattern()),
        );
        ```
        """
        ...

    def match_kind(self, kind) -> PyAhoCorasickBuilder:
        r"""
        Set the desired match semantics.
        
        The default is [`PyMatchKind::Standard`], which corresponds to the match
        semantics supported by the standard textbook description of the
        Aho-Corasick algorithm. Namely, matches are reported as soon as they
        are found. Moreover, this is the only way to get overlapping matches
        or do stream searching.
        
        The other kinds of match semantics that are supported are
        [`PyMatchKind::LeftmostFirst`] and [`PyMatchKind::LeftmostLongest`]. The
        former corresponds to the match you would get if you were to try to
        match each pattern at each position in the haystack in the same order
        that you give to the automaton. That is, it returns the leftmost match
        corresponding to the earliest pattern given to the automaton. The
        latter corresponds to finding the longest possible match among all
        leftmost matches.
        
        For more details on match semantics, see the [documentation for
        `MatchKind`](MatchKind).
        
        Note that setting this to [`PyMatchKind::LeftmostFirst`] or
        [`PyMatchKind::LeftmostLongest`] will cause some search routines on
        [`PyAhoCorasick`] to return an error (or panic if you're using the
        infallible API). Notably, this includes stream and overlapping
        searches.
        
        # Examples
        
        In these examples, we demonstrate the differences between match
        semantics for a particular set of patterns in a specific order:
        `b`, `abc`, `abcd`.
        
        Standard semantics:
        
        ```plaintext
        use ldatranslate::py::tokenizer::{PyAhoCorasick, PyMatchKind};
        
        let patterns = &["b", "abc", "abcd"];
        let haystack = "abcd";
        
        let ac = PyAhoCorasick::builder()
            .match_kind(PyMatchKind::Standard) // default, not necessary
            .build(patterns)
            .unwrap();
        let mat = ac.find(haystack).expect("should have a match");
        assert_eq!("b", &haystack[mat.start()..mat.end()]);
        ```
        
        Leftmost-first semantics:
        
        ```plaintext
        use ldatranslate::py::tokenizer::{PyAhoCorasick, PyMatchKind};
        
        let patterns = &["b", "abc", "abcd"];
        let haystack = "abcd";
        
        let ac = PyAhoCorasick::builder()
            .match_kind(PyMatchKind::LeftmostFirst)
            .build(patterns)
            .unwrap();
        let mat = ac.find(haystack).expect("should have a match");
        assert_eq!("abc", &haystack[mat.start()..mat.end()]);
        ```
        
        Leftmost-longest semantics:
        
        ```plaintext
        use ldatranslate::py::tokenizer::{PyAhoCorasick, PyMatchKind};
        
        let patterns = &["b", "abc", "abcd"];
        let haystack = "abcd";
        
        let ac = PyAhoCorasick::builder()
            .match_kind(PyMatchKind::LeftmostLongest)
            .build(patterns)
            .unwrap();
        let mat = ac.find(haystack).expect("should have a match");
        assert_eq!("abcd", &haystack[mat.start()..mat.end()]);
        ```
        """
        ...

    def start_kind(self, kind) -> PyAhoCorasickBuilder:
        r"""
        Sets the starting state configuration for the automaton.
        
        Every Aho-Corasick automaton is capable of having two start states: one
        that is used for unanchored searches and one that is used for anchored
        searches. Some automatons, like the NFAs, support this with almost zero
        additional cost. Other automatons, like the DFA, require two copies of
        the underlying transition table to support both simultaneously.
        
        Because there may be an added non-trivial cost to supporting both, it
        is possible to configure which starting state configuration is needed.
        
        Indeed, since anchored searches tend to be somewhat more rare,
        _only_ unanchored searches are supported by default. Thus,
        [`PyStartKind::Unanchored`] is the default.
        
        Note that when this is set to [`PyStartKind::Unanchored`], then
        running an anchored search will result in an error (or a panic
        if using the infallible APIs). Similarly, when this is set to
        [`PyStartKind::Anchored`], then running an unanchored search will
        result in an error (or a panic if using the infallible APIs). When
        [`PyStartKind::Both`] is used, then both unanchored and anchored searches
        are always supported.
        
        Also note that even if an `PyAhoCorasick` searcher is using an NFA
        internally (which always supports both unanchored and anchored
        searches), an error will still be reported for a search that isn't
        supported by the configuration set via this method. This means,
        for example, that an error is never dependent on which internal
        implementation of Aho-Corasick is used.
        
        # Example: anchored search
        
        This shows how to build a searcher that only supports anchored
        searches:
        
        ```
        use aho_corasick::{
            AhoCorasick, Anchored, Input, Match, MatchKind, StartKind,
        };
        
        let ac = AhoCorasick::builder()
            .match_kind(MatchKind::LeftmostFirst)
            .start_kind(StartKind::Anchored)
            .build(&["b", "abc", "abcd"])
            .unwrap();
        
        // An unanchored search is not supported! An error here is guaranteed
        // given the configuration above regardless of which kind of
        // Aho-Corasick implementation ends up being used internally.
        let input = Input::new("foo abcd").anchored(Anchored::No);
        assert!(ac.try_find(input).is_err());
        
        let input = Input::new("foo abcd").anchored(Anchored::Yes);
        assert_eq!(None, ac.try_find(input)?);
        
        let input = Input::new("abcd").anchored(Anchored::Yes);
        assert_eq!(Some(Match::must(1, 0..3)), ac.try_find(input)?);
        
        # Ok::<(), Box<dyn std::error::Error>>(())
        ```
        
        # Example: unanchored and anchored searches
        
        This shows how to build a searcher that supports both unanchored and
        anchored searches:
        
        ```
        use aho_corasick::{
            AhoCorasick, Anchored, Input, Match, MatchKind, StartKind,
        };
        
        let ac = AhoCorasick::builder()
            .match_kind(MatchKind::LeftmostFirst)
            .start_kind(StartKind::Both)
            .build(&["b", "abc", "abcd"])
            .unwrap();
        
        let input = Input::new("foo abcd").anchored(Anchored::No);
        assert_eq!(Some(Match::must(1, 4..7)), ac.try_find(input)?);
        
        let input = Input::new("foo abcd").anchored(Anchored::Yes);
        assert_eq!(None, ac.try_find(input)?);
        
        let input = Input::new("abcd").anchored(Anchored::Yes);
        assert_eq!(Some(Match::must(1, 0..3)), ac.try_find(input)?);
        
        # Ok::<(), Box<dyn std::error::Error>>(())
        ```
        """
        ...

    def ascii_case_insensitive(self, yes) -> PyAhoCorasickBuilder:
        r"""
        Enable ASCII-aware case insensitive matching.
        
        When this option is enabled, searching will be performed without
        respect to case for ASCII letters (`a-z` and `A-Z`) only.
        
        Enabling this option does not change the search algorithm, but it may
        increase the size of the automaton.
        
        **NOTE:** It is unlikely that support for Unicode case folding will
        be added in the future. The ASCII case works via a simple hack to the
        underlying automaton, but full Unicode handling requires a fair bit of
        sophistication. If you do need Unicode handling, you might consider
        using the [`regex` crate](https://docs.rs/regex) or the lower level
        [`regex-automata` crate](https://docs.rs/regex-automata).
        
        # Examples
        
        Basic usage:
        
        ```
        use aho_corasick::AhoCorasick;
        
        let patterns = &["FOO", "bAr", "BaZ"];
        let haystack = "foo bar baz";
        
        let ac = AhoCorasick::builder()
            .ascii_case_insensitive(true)
            .build(patterns)
            .unwrap();
        assert_eq!(3, ac.find_iter(haystack).count());
        ```
        """
        ...

    def kind(self, kind = ...) -> PyAhoCorasickBuilder:
        r"""
        Choose the type of underlying automaton to use.
        
        Currently, there are four choices:
        
        * [`PyAhoCorasickKind::NoncontiguousNFA`] instructs the searcher to
        use a [`noncontiguous::NFA`]. A noncontiguous NFA is the fastest to
        be built, has moderate memory usage and is typically the slowest to
        execute a search.
        * [`PyAhoCorasickKind::ContiguousNFA`] instructs the searcher to use a
        [`contiguous::NFA`]. A contiguous NFA is a little slower to build than
        a noncontiguous NFA, has excellent memory usage and is typically a
        little slower than a DFA for a search.
        * [`PyAhoCorasickKind::DFA`] instructs the searcher to use a
        [`dfa::DFA`]. A DFA is very slow to build, uses exorbitant amounts of
        memory, but will typically execute searches the fastest.
        * `None` (the default) instructs the searcher to choose the "best"
        Aho-Corasick implementation. This choice is typically based primarily
        on the number of patterns.
        
        Setting this configuration does not change the time complexity for
        constructing the Aho-Corasick automaton (which is `O(p)` where `p`
        is the total number of patterns being compiled). Setting this to
        [`PyAhoCorasickKind::DFA`] does however reduce the time complexity of
        non-overlapping searches from `O(n + p)` to `O(n)`, where `n` is the
        length of the haystack.
        
        In general, you should probably stick to the default unless you have
        some kind of reason to use a specific Aho-Corasick implementation. For
        example, you might choose `PyAhoCorasickKind::DFA` if you don't care
        about memory usage and want the fastest possible search times.
        
        Setting this guarantees that the searcher returned uses the chosen
        implementation. If that implementation could not be constructed, then
        an error will be returned. In contrast, when `None` is used, it is
        possible for it to attempt to construct, for example, a contiguous
        NFA and have it fail. In which case, it will fall back to using a
        noncontiguous NFA.
        
        If `None` is given, then one may use [`PyAhoCorasickKind::kind`] to determine
        which Aho-Corasick implementation was chosen.
        
        Note that the heuristics used for choosing which `PyAhoCorasickKind`
        may be changed in a semver compatible release.
        """
        ...

    def prefilter(self, yes) -> PyAhoCorasickBuilder:
        r"""
        Enable heuristic prefilter optimizations.
        
        When enabled, searching will attempt to quickly skip to match
        candidates using specialized literal search routines. A prefilter
        cannot always be used, and is generally treated as a heuristic. It
        can be useful to disable this if the prefilter is observed to be
        sub-optimal for a particular workload.
        
        Currently, prefilters are typically only active when building searchers
        with a small (less than 100) number of patterns.
        
        This is enabled by default.
        """
        ...

    def dense_depth(self, depth) -> PyAhoCorasickBuilder:
        r"""
        Set the limit on how many states use a dense representation for their
        transitions. Other states will generally use a sparse representation.
        
        A dense representation uses more memory but is generally faster, since
        the next transition in a dense representation can be computed in a
        constant number of instructions. A sparse representation uses less
        memory but is generally slower, since the next transition in a sparse
        representation requires executing a variable number of instructions.
        
        This setting is only used when an Aho-Corasick implementation is used
        that supports the dense versus sparse representation trade off. Not all
        do.
        
        This limit is expressed in terms of the depth of a state, i.e., the
        number of transitions from the starting state of the automaton. The
        idea is that most of the time searching will be spent near the starting
        state of the automaton, so states near the start state should use a
        dense representation. States further away from the start state would
        then use a sparse representation.
        
        By default, this is set to a low but non-zero number. Setting this to
        `0` is almost never what you want, since it is likely to make searches
        very slow due to the start state itself being forced to use a sparse
        representation. However, it is unlikely that increasing this number
        will help things much, since the most active states have a small depth.
        More to the point, the memory usage increases superlinearly as this
        number increases.
        """
        ...

    def byte_classes(self, yes) -> PyAhoCorasickBuilder:
        r"""
        A debug setting for whether to attempt to shrink the size of the
        automaton's alphabet or not.
        
        This option is enabled by default and should never be disabled unless
        one is debugging the underlying automaton.
        
        When enabled, some (but not all) Aho-Corasick automatons will use a map
        from all possible bytes to their corresponding equivalence class. Each
        equivalence class represents a set of bytes that does not discriminate
        between a match and a non-match in the automaton.
        
        The advantage of this map is that the size of the transition table can
        be reduced drastically from `#states * 256 * sizeof(u32)` to
        `#states * k * sizeof(u32)` where `k` is the number of equivalence
        classes (rounded up to the nearest power of 2). As a result, total
        space usage can decrease substantially. Moreover, since a smaller
        alphabet is used, automaton compilation becomes faster as well.
        
        **WARNING:** This is only useful for debugging automatons. Disabling
        this does not yield any speed advantages. Namely, even when this is
        disabled, a byte class map is still used while searching. The only
        difference is that every byte will be forced into its own distinct
        equivalence class. This is useful for debugging the actual generated
        transitions because it lets one see the transitions defined on actual
        bytes instead of the equivalence classes.
        """
        ...


class PyAlignedArticle:
    article_id: int
    language_hints: list[LanguageHint]
    def __new__(cls,article_id:int, articles:typing.Mapping[str | LanguageHint, PyArticle]): ...
    @staticmethod
    def create(article_id:int, articles:dict[str | LanguageHint, PyArticle] | list[PyArticle]) -> PyAlignedArticle | tuple[PyAlignedArticle, list[PyArticle]]:
        ...

    def __str__(self) -> str:
        ...

    def __repr__(self) -> str:
        ...

    def __getitem__(self, item:str | LanguageHint) -> typing.Optional[PyArticle]:
        ...

    def __contains__(self, item:str | LanguageHint) -> bool:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyAlignedArticle:
        ...


class PyAlignedArticleIter:
    def __iter__(self) -> PyAlignedArticleIter:
        ...

    def __next__(self) -> typing.Optional[PyAlignedArticle]:
        ...


class PyAlignedArticleParsedIter:
    def __iter__(self) -> PyAlignedArticleParsedIter:
        ...

    def __next__(self) -> typing.Optional[PyTokenizedAlignedArticle]:
        ...


class PyAlignedArticleProcessor:
    def __new__(cls,processors:typing.Mapping[str | LanguageHint, PyTokenizerBuilder]): ...
    def __getitem__(self, value:str | LanguageHint) -> typing.Optional[PyTokenizerBuilder]:
        ...

    def process(self, value:PyAlignedArticle) -> PyTokenizedAlignedArticle:
        ...

    def __contains__(self, language_hint:str | LanguageHint) -> bool:
        ...

    def process_string(self, language_hint:str | LanguageHint, value:str) -> typing.Optional[list[tuple[str, PyToken]]]:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyAlignedArticleProcessor:
        ...


class PyArticle:
    py_is_list: bool
    py_lang: LanguageHint
    py_categories: typing.Optional[list[int]]
    py_content: typing.Optional[str]
    def __new__(cls,language_hint,content,categories = ...,is_list = ...): ...
    def __str__(self) -> str:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyArticle:
        ...


class PyClassifierOption:
    stop_words: typing.Optional[PyStopWords]
    separators: typing.Optional[list[str]]
    def __new__(cls,): ...
    def set_separators(self, value:typing.Optional[typing.Sequence[str]]) -> None:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyClassifierOption:
        ...


class PyContextWithMutableVariables:
    r"""
    This is an unsafe reference to a VotingMethodContext.
    If a python user saves them outside of the method, there will be a memory error.
    """
    def __getitem__(self, item:str) -> str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]:
        ...

    def __setitem__(self, key:str, value:str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]) -> None:
        ...

    def __contains__(self, item:str) -> bool:
        ...

    def get_all_values(self) -> dict[str, str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]]:
        ...


class PyDictIter:
    def __iter__(self) -> PyDictIter:
        ...

    def __next__(self) -> typing.Optional[tuple[tuple[int, str, typing.Optional[LoadedMetadataEx]], tuple[int, str, typing.Optional[LoadedMetadataEx]], DirectionKind]]:
        ...


class PyDictionary:
    known_dictionaries: list[str]
    translation_direction: tuple[typing.Optional[LanguageHint], typing.Optional[LanguageHint]]
    voc_a_py: PyVocabulary
    voc_b_py: PyVocabulary
    def __new__(cls,language_a:typing.Optional[str | LanguageHint], language_b:typing.Optional[str | LanguageHint]): ...
    def set_translation_direction(self, option:tuple[typing.Optional[str | LanguageHint], typing.Optional[str | LanguageHint]]) -> None:
        r"""
        Allows to set the translation languages. This is usually not necessary, except youbuild your own.
        """
        ...

    def voc_a_contains(self, value:str) -> bool:
        r"""
        Returns true if voc a contains the value
        """
        ...

    def voc_b_contains(self, value:str) -> bool:
        r"""
        Returns true if voc a contains the value
        """
        ...

    def __contains__(self, value:str) -> bool:
        r"""
        Returns true if any voc contains the value
        """
        ...

    def switch_a_to_b(self) -> PyDictionary:
        ...

    def add(self, word_a:tuple[str, typing.Optional[LoadedMetadataEx]], word_b:tuple[str, typing.Optional[LoadedMetadataEx]]) -> tuple[int, int, DirectionKind]:
        r"""
        Insert a translation from word a to b with
        """
        ...

    def get_translation_a_to_b(self, word:str) -> typing.Optional[list[str]]:
        r"""
        Returns the translations of the word, from a to b
        """
        ...

    def get_translation_b_to_a(self, word:str) -> typing.Optional[list[str]]:
        r"""
        Returns the translations of the word, from b to a
        """
        ...

    def __repr__(self) -> str:
        ...

    def __str__(self) -> str:
        ...

    def save(self, path:str | os.PathLike | pathlib.Path, mode:typing.Literal["b", "binary", "b+c", "binary+compressed", "json", "j", "json+compressed", "j+c", "json+pretty", "j+p", "json+pretty+compressed", "j+p+c"]) -> None:
        r"""
        Writes the dictionary to the path, the mode is chosen based on the file ending.
        """
        ...

    def save_as(self, path:str | os.PathLike | pathlib.Path, mode:typing.Literal["b", "binary", "b+c", "binary+compressed", "json", "j", "json+compressed", "j+c", "json+pretty", "j+p", "json+pretty+compressed", "j+p+c"]) -> None:
        r"""
        Writes the dictionary to the path with the chosen mode
        """
        ...

    @staticmethod
    def load(path:str | os.PathLike | pathlib.Path) -> PyDictionary:
        r"""
        Loads the dictionary from the path with the provided mode
        """
        ...

    @staticmethod
    def load_as(path:str | os.PathLike | pathlib.Path, mode:typing.Literal["b", "binary", "b+c", "binary+compressed", "json", "j", "json+compressed", "j+c", "json+pretty", "j+p", "json+pretty+compressed", "j+p+c"]) -> PyDictionary:
        r"""
        Loads the dictionary from the path with the provided mode
        """
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyDictionary:
        ...

    def __iter__(self) -> PyDictIter:
        ...

    def filter(self, filter_a:typing.Callable[[str, typing.Optional[LoadedMetadataEx]], bool], filter_b:typing.Callable[[str, typing.Optional[LoadedMetadataEx]], bool]) -> PyDictionary:
        r"""
        Filters a dictionary by the defined methods and returns a new instance.
        """
        ...

    def get_meta_a_of(self, word:str) -> typing.Optional[LoadedMetadataEx]:
        r"""
        Returns the meta for a specific word in a
        """
        ...

    def get_meta_b_of(self, word:str) -> typing.Optional[LoadedMetadataEx]:
        r"""
        Returns the meta for a specific word in b
        """
        ...

    def process_with_tokenizer(self, processor:PyAlignedArticleProcessor) -> PyDictionary:
        r"""
        Creates a new dictionary where the processor was applied.
        Requires that both languages (a+b) are properly set.
        """
        ...

    def create_html_view_in(self, target:str | os.PathLike | pathlib.Path) -> None:
        r"""
        Creates an html view of the vocabulary in this folder.
        """
        ...

    def len(self) -> Len:
        r"""
        Returns a len object, containing the counts of the single parts
        """
        ...


class PyNormalizerOption:
    create_char_map: bool
    classifier: PyClassifierOption
    lossy: bool
    def __new__(cls,): ...
    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyNormalizerOption:
        ...


class PyParsedAlignedArticleIter:
    def __iter__(self) -> PyParsedAlignedArticleIter:
        ...

    def __next__(self) -> typing.Optional[PyTokenizedAlignedArticle]:
        ...


class PySegmenterOption:
    aho: typing.Optional[PyAhoCorasick]
    allow_list: typing.Optional[dict[PyScript, list[PyLanguage]]]
    def __new__(cls,): ...
    def set_allow_list(self, allow_list:typing.Optional[typing.Mapping[PyScript, typing.Sequence[PyLanguage]]]) -> None:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PySegmenterOption:
        ...


class PyStopWords:
    def __new__(cls,words:set[str] | list[str]): ...
    def __contains__(self, value:str) -> bool:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyStopWords:
        ...


class PyToken:
    kind: PyTokenKind
    lemma: str
    char_start: int
    char_end: int
    byte_start: int
    byte_end: int
    char_map: typing.Optional[list[tuple[int, int]]]
    script: PyScript
    language: typing.Optional[PyLanguage]
    def byte_len(self) -> int:
        ...

    def __len__(self) -> int:
        ...

    def __str__(self) -> str:
        ...

    def __repr__(self) -> str:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyToken:
        ...


class PyTokenizedAlignedArticle:
    article_id: int
    language_hints: list[LanguageHint]
    def __new__(cls,article_id:int, articles:typing.Mapping[str | LanguageHint, PyArticle | list[tuple[str, PyToken]]]): ...
    @staticmethod
    def create(article_id:int, articles:dict[str | LanguageHint, PyArticle | list[tuple[str, PyToken]]] | list[PyArticle | list[tuple[str, PyToken]]]) -> PyTokenizedAlignedArticle | tuple[PyTokenizedAlignedArticle, list[PyArticle | list[tuple[str, PyToken]]]]:
        ...

    def __str__(self) -> str:
        ...

    def __repr__(self) -> str:
        ...

    def __getitem__(self, item:str | LanguageHint) -> typing.Optional[PyArticle | list[tuple[str, PyToken]]]:
        ...

    def __contains__(self, item:str | LanguageHint) -> bool:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyTokenizedAlignedArticle:
        ...


class PyTokenizerBuilder:
    def __new__(cls,): ...
    def stemmer(self, stemmer,smart = ...) -> PyTokenizerBuilder:
        ...

    def phrase_vocabulary(self, vocabulary) -> PyTokenizerBuilder:
        ...

    def stop_words(self, stop_words) -> PyTokenizerBuilder:
        ...

    def separators(self, separators) -> PyTokenizerBuilder:
        ...

    def words_dict(self, words) -> PyTokenizerBuilder:
        ...

    def create_char_map(self, create_char_map) -> PyTokenizerBuilder:
        ...

    def lossy_normalization(self, lossy) -> PyTokenizerBuilder:
        ...

    def unicode_segmentation(self, unicode) -> PyTokenizerBuilder:
        ...

    def allow_list(self, allow_list) -> PyTokenizerBuilder:
        ...

    def create_stopword_filter(self) -> typing.Optional[PyStopWords]:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyTokenizerBuilder:
        ...

    def __repr__(self) -> str:
        ...


class PyTopicModel:
    py_k: int
    def __new__(cls,topics:typing.Sequence[typing.Sequence[float]], vocabulary:PyVocabulary, used_vocab_frequency:typing.Sequence[int], doc_topic_distributions:typing.Sequence[typing.Sequence[float]], document_lengths:typing.Sequence[int]): ...
    def get_topic(self, topic_id:int) -> typing.Optional[list[float]]:
        ...

    def save(self, path:str | os.PathLike | pathlib.Path) -> int:
        ...

    @staticmethod
    def load(path:str | os.PathLike | pathlib.Path) -> PyTopicModel:
        ...

    def show_top(self, n = ...) -> None:
        ...

    def __repr__(self) -> str:
        ...

    def __str__(self) -> str:
        ...

    def get_doc_probability(self, doc,alpha,gamma_threshold,minimum_probability = ...,minimum_phi_value = ...,per_word_topics = ...) -> tuple[list[tuple[int, float]], typing.Optional[list[tuple[int, list[int]]]], typing.Optional[list[tuple[int, list[tuple[int, float]]]]]]:
        ...

    def vocabulary(self) -> PyVocabulary:
        ...

    def get_words_of_topic_sorted(self, topic_id:int) -> typing.Optional[list[tuple[str, float]]]:
        ...

    def get_topic_as_words(self, topic_id:int) -> typing.Optional[list[tuple[int, str, float]]]:
        ...

    def translate_by_provided_word_lists(self, language_hint:str | LanguageHint, word_lists:list[str] | list[list[str]]) -> PyTopicModel:
        ...

    def save_json(self, path:str | os.PathLike | pathlib.Path) -> None:
        ...

    def save_binary(self, path:str | os.PathLike | pathlib.Path) -> None:
        ...

    @staticmethod
    def load_json(path:str | os.PathLike | pathlib.Path) -> PyTopicModel:
        ...

    @staticmethod
    def load_binary(path:str | os.PathLike | pathlib.Path) -> PyTopicModel:
        ...

    def normalize(self) -> PyTopicModel:
        ...

    @staticmethod
    def builder(language = ...) -> PyTopicModelBuilder:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> PyTopicModel:
        ...


class PyTopicModelBuilder:
    def __new__(cls,language = ...): ...
    def set_frequency(self, word:str, frequency:int) -> None:
        ...

    def add_word(self, topic_id,word,probability,frequency = ...) -> None:
        ...

    def set_doc_topic_distributions(self, doc_topic_distributions = ...) -> None:
        ...

    def set_document_lengths(self, document_lengths = ...) -> None:
        ...

    def build(self, unset_words_become_smallest = ...,normalize = ...) -> PyTopicModel:
        ...


class PyTranslationConfig:
    def __new__(cls,epsilon = ...,threshold = ...,keep_original_word = ...,top_candidate_limit = ...): ...
    ...

class PyVariableProvider:
    def __new__(cls,): ...
    def add_global(self, key:str, value:str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]) -> typing.Optional[str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]]:
        ...

    def add_for_topic(self, topic_id:int, key:str, value:str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]) -> None:
        ...

    def add_for_word_a(self, word:str, key:str, value:str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]) -> None:
        ...

    def add_for_word_b(self, word:str, key:str, value:str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]) -> None:
        ...

    def add_for_word_in_topic_a(self, topic_id:int, word:str, key:str, value:str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]) -> None:
        ...

    def add_for_word_in_topic_b(self, topic_id:int, word:str, key:str, value:str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]]) -> None:
        ...


class PyVocIter:
    def __iter__(self) -> PyVocIter:
        ...

    def __next__(self) -> typing.Optional[str]:
        ...


class PyVocabulary:
    language_hint: typing.Optional[LanguageHint]
    def __new__(cls,language = ...,size = ...): ...
    def set_language(self, value:typing.Optional[str | LanguageHint]) -> None:
        ...

    def __repr__(self) -> str:
        ...

    def __str__(self) -> str:
        ...

    def __len__(self) -> int:
        ...

    def __contains__(self, value:str) -> bool:
        ...

    def __iter__(self) -> PyVocIter:
        ...

    def add(self, word:str) -> int:
        ...

    def word_to_id(self, word:str) -> typing.Optional[int]:
        ...

    def id_to_word(self, id:int) -> typing.Optional[str]:
        ...

    def save(self, path:str | os.PathLike | pathlib.Path) -> int:
        r"""
        Save the vocabulary in a standardisized way
        """
        ...

    @staticmethod
    def load(path:str | os.PathLike | pathlib.Path) -> PyVocabulary:
        r"""
        Load the vocabulary from a file
        """
        ...

    def to_json(self) -> str:
        r"""
        Serializes this to a json
        """
        ...

    @staticmethod
    def from_json(s:str) -> PyVocabulary:
        r"""
        Deserializes a json to a vocabulary.
        """
        ...


class PyVoting:
    @staticmethod
    def parse(value,registry = ...) -> PyVoting:
        ...

    def __call__(self, global_context:PyContextWithMutableVariables, voters:typing.Sequence[PyContextWithMutableVariables]) -> tuple[str | float | int | bool | None | list[PyExprValueSingle] | list[str | float | int | bool | None | list[PyExprValueSingle]], list[PyContextWithMutableVariables]]:
        ...


class PyVotingRegistry:
    def __new__(cls,): ...
    def get_registered(self, name:str) -> typing.Optional[PyVoting]:
        ...

    def register_at(self, name:str, voting:str) -> None:
        ...

    def register(self, voting:str) -> None:
        ...


class StoreOptions:
    r"""
    The options for storing.
    """
    deflate_temp_files: bool
    delete_temp_files_immediately: bool
    compress_result: bool
    show_progress_after: typing.Optional[int]
    temp_folder: typing.Optional[str]
    def __new__(cls,deflate_temp_files = ...,delete_temp_files_immediately = ...,compress_result = ...,temp_folder = ...,show_progress_after = ...): ...
    def show_progress_after(self, show_progress_after:int) -> None:
        ...

    def temp_folder(self, temp_folder:typing.Optional[str | os.PathLike | pathlib.Path]) -> None:
        ...


class TokenCountFilter:
    min: typing.Optional[int]
    max: typing.Optional[int]
    def __new__(cls,min = ...,max = ...): ...
    def set_min(self, min:typing.Optional[int]) -> None:
        ...

    def set_max(self, max:typing.Optional[int]) -> None:
        ...

    def to_json(self) -> str:
        ...

    @staticmethod
    def from_json(s:str) -> TokenCountFilter:
        ...

    def __contains__(self, value:int) -> bool:
        ...


class BuildInVoting(Enum):
    r"""
    All possible buildin votings
    """
    OriginalScore = auto()
    Voters = auto()
    CombSum = auto()
    GCombSum = auto()
    CombSumTop = auto()
    CombSumPow2 = auto()
    CombMax = auto()
    RR = auto()
    RRPow2 = auto()
    CombSumRR = auto()
    CombSumRRPow2 = auto()
    CombSumPow2RR = auto()
    CombSumPow2RRPow2 = auto()
    ExpCombMnz = auto()
    WCombSum = auto()
    WCombSumG = auto()
    WGCombSum = auto()
    PCombSum = auto()

class DictionaryLanguage(Enum):
    r"""
    The recognized
    """
    English = auto()
    German = auto()
    Italian = auto()
    French = auto()
    Latin = auto()

class DirectionKind(Enum):
    r"""
    The direction of the language
    """
    AToB = auto()
    BToA = auto()
    Invariant = auto()

class Domain(Enum):
    Acad = auto()
    Acc = auto()
    Admin = auto()
    Agr = auto()
    Anat = auto()
    Archaeo = auto()
    Archi = auto()
    Armour = auto()
    Art = auto()
    Astrol = auto()
    Astron = auto()
    Astronau = auto()
    Audio = auto()
    Automot = auto()
    Aviat = auto()
    Bibl = auto()
    Bike = auto()
    Biochem = auto()
    Biol = auto()
    Biotech = auto()
    Bot = auto()
    Brew = auto()
    Chem = auto()
    Climbing = auto()
    Cloth = auto()
    Comics = auto()
    Comm = auto()
    Comp = auto()
    Constr = auto()
    Cook = auto()
    Cosmet = auto()
    Curr = auto()
    Dance = auto()
    Dent = auto()
    Drugs = auto()
    Ecol = auto()
    Econ = auto()
    Educ = auto()
    Electr = auto()
    Engin = auto()
    Entom = auto()
    Equest = auto()
    Esot = auto()
    Ethn = auto()
    Eu = auto()
    F = auto()
    Film = auto()
    Fin = auto()
    FireResc = auto()
    Fish = auto()
    FoodInd = auto()
    For = auto()
    Furn = auto()
    Games = auto()
    Gastr = auto()
    Geogr = auto()
    Geol = auto()
    Herald = auto()
    Hist = auto()
    Hort = auto()
    Hunting = auto()
    Hydro = auto()
    Idiom = auto()
    Ind = auto()
    Insur = auto()
    Internet = auto()
    Jobs = auto()
    Journ = auto()
    Law = auto()
    Libr = auto()
    Ling = auto()
    Lit = auto()
    Mach = auto()
    Market = auto()
    Material = auto()
    Math = auto()
    Med = auto()
    MedTech = auto()
    Meteo = auto()
    Mil = auto()
    Mineral = auto()
    Mining = auto()
    Mus = auto()
    Mycol = auto()
    Myth = auto()
    Name = auto()
    Naut = auto()
    Neol = auto()
    Nucl = auto()
    Oenol = auto()
    Optics = auto()
    Orn = auto()
    Pharm = auto()
    Philat = auto()
    Philos = auto()
    Phonet = auto()
    Photo = auto()
    Phys = auto()
    Pol = auto()
    Print = auto()
    Proverb = auto()
    Psych = auto()
    Publ = auto()
    Qm = auto()
    Quote = auto()
    RadioTv = auto()
    Rail = auto()
    RealEst = auto()
    Relig = auto()
    Rhet = auto()
    School = auto()
    Sociol = auto()
    Spec = auto()
    Sports = auto()
    Stat = auto()
    Stocks = auto()
    Stud = auto()
    T = auto()
    Tech = auto()
    Telecom = auto()
    Textil = auto()
    Theatre = auto()
    Tools = auto()
    Toys = auto()
    TrVocab = auto()
    Traffic = auto()
    Transp = auto()
    Travel = auto()
    Unit = auto()
    Urban = auto()
    Uwh = auto()
    VetMed = auto()
    Watches = auto()
    Weapons = auto()
    Zool = auto()
    Child = auto()
    Youth = auto()
    Science = auto()
    Poetry = auto()
    Currency = auto()
    Phila = auto()
    Commun = auto()
    Media = auto()
    Tour = auto()
    Alchemy = auto()
    Anime = auto()
    Bever = auto()
    Sex = auto()
    Palaeo = auto()
    Metal = auto()
    Masonry = auto()
    Colour = auto()
    Mechanics = auto()
    Money = auto()
    NatSci = auto()
    PseudoSci = auto()
    Humanities = auto()

class GrammaticalGender(Enum):
    Feminine = auto()
    Masculine = auto()
    Neutral = auto()
    NotFeminine = auto()

class GrammaticalNumber(Enum):
    Singular = auto()
    Plural = auto()
    NoSingular = auto()
    NoPlural = auto()
    UsuallyPlural = auto()
    SingularOnly = auto()
    PluralOnly = auto()

class KeepOriginalWord(Enum):
    r"""
    Setting if to keep the original word from language A
    """
    Always = auto()
    IfNoTranslation = auto()
    Never = auto()

class LanguageKind(Enum):
    r"""
    The language
    """
    A = auto()
    B = auto()

class MetaField(Enum):
    Languages = auto()
    Domains = auto()
    Registers = auto()
    Genders = auto()
    Pos = auto()
    PosTag = auto()
    Regions = auto()
    Numbers = auto()
    InternalIds = auto()
    Inflected = auto()
    Abbreviations = auto()
    UnalteredVocabulary = auto()
    LookAt = auto()
    Ids = auto()
    OutgoingIds = auto()
    OriginalEntry = auto()
    ContextualInformations = auto()
    Unclassified = auto()
    Synonyms = auto()

class PartOfSpeech(Enum):
    Noun = auto()
    Verb = auto()
    Adv = auto()
    Pron = auto()
    Conj = auto()
    Prep = auto()
    Det = auto()
    Intj = auto()
    Num = auto()
    Article = auto()
    Name = auto()
    Prefix = auto()
    Suffix = auto()
    Character = auto()
    Particle = auto()
    Other = auto()
    Abbrev = auto()
    Adj = auto()
    AdjNoun = auto()
    AdjVerb = auto()
    Adnominal = auto()
    AdvPhrase = auto()
    Affix = auto()
    Ambiposition = auto()
    Circumfix = auto()
    Circumpos = auto()
    Classifier = auto()
    Clause = auto()
    CombiningForm = auto()
    Contraction = auto()
    Converb = auto()
    Counter = auto()
    Infix = auto()
    Interfix = auto()
    Phrase = auto()
    Postp = auto()
    PrepPhrase = auto()
    Preverb = auto()
    Proverb = auto()
    Punct = auto()
    Romanization = auto()
    Root = auto()
    Syllable = auto()
    Symbol = auto()

class PartOfSpeechTag(Enum):
    Morpheme = auto()
    Participle = auto()
    Present = auto()
    Transitive = auto()
    Ideophone = auto()
    Perfect = auto()
    Verbal = auto()
    Gerund = auto()
    Infinitive = auto()
    Interrogative = auto()
    Intransitive = auto()
    Past = auto()
    Person = auto()
    Prepositional = auto()
    Relative = auto()
    Hanja = auto()
    Abbreviation = auto()
    Clitic = auto()
    Comparative = auto()
    Dependent = auto()
    Diacritic = auto()
    Han = auto()
    Hanzi = auto()
    Idiomatic = auto()
    Indefinite = auto()
    Kanji = auto()
    Letter = auto()
    Ligature = auto()
    Number = auto()
    Ordinal = auto()
    Possessive = auto()
    Predicative = auto()
    Punctuation = auto()

class PyAhoCorasickKind(Enum):
    NoncontiguousNFA = auto()
    ContiguousNFA = auto()
    DFA = auto()

class PyLanguage(Enum):
    Epo = auto()
    Eng = auto()
    Rus = auto()
    Cmn = auto()
    Spa = auto()
    Por = auto()
    Ita = auto()
    Ben = auto()
    Fra = auto()
    Deu = auto()
    Ukr = auto()
    Kat = auto()
    Ara = auto()
    Hin = auto()
    Jpn = auto()
    Heb = auto()
    Yid = auto()
    Pol = auto()
    Amh = auto()
    Jav = auto()
    Kor = auto()
    Nob = auto()
    Dan = auto()
    Swe = auto()
    Fin = auto()
    Tur = auto()
    Nld = auto()
    Hun = auto()
    Ces = auto()
    Ell = auto()
    Bul = auto()
    Bel = auto()
    Mar = auto()
    Kan = auto()
    Ron = auto()
    Slv = auto()
    Hrv = auto()
    Srp = auto()
    Mkd = auto()
    Lit = auto()
    Lav = auto()
    Est = auto()
    Tam = auto()
    Vie = auto()
    Urd = auto()
    Tha = auto()
    Guj = auto()
    Uzb = auto()
    Pan = auto()
    Aze = auto()
    Ind = auto()
    Tel = auto()
    Pes = auto()
    Mal = auto()
    Ori = auto()
    Mya = auto()
    Nep = auto()
    Sin = auto()
    Khm = auto()
    Tuk = auto()
    Aka = auto()
    Zul = auto()
    Sna = auto()
    Afr = auto()
    Lat = auto()
    Slk = auto()
    Cat = auto()
    Tgl = auto()
    Hye = auto()
    Other = auto()

class PyMatchKind(Enum):
    Standard = auto()
    LeftmostFirst = auto()
    LeftmostLongest = auto()

class PyScript(Enum):
    Arabic = auto()
    Armenian = auto()
    Bengali = auto()
    Cyrillic = auto()
    Devanagari = auto()
    Ethiopic = auto()
    Georgian = auto()
    Greek = auto()
    Gujarati = auto()
    Gurmukhi = auto()
    Hangul = auto()
    Hebrew = auto()
    Kannada = auto()
    Khmer = auto()
    Latin = auto()
    Malayalam = auto()
    Myanmar = auto()
    Oriya = auto()
    Sinhala = auto()
    Tamil = auto()
    Telugu = auto()
    Thai = auto()
    Cj = auto()
    Other = auto()

class PyStartKind(Enum):
    Both = auto()
    Unanchored = auto()
    Anchored = auto()

class PyStemmingAlgorithm(Enum):
    Arabic = auto()
    Danish = auto()
    Dutch = auto()
    English = auto()
    Finnish = auto()
    French = auto()
    German = auto()
    Greek = auto()
    Hungarian = auto()
    Italian = auto()
    Norwegian = auto()
    Portuguese = auto()
    Romanian = auto()
    Russian = auto()
    Spanish = auto()
    Swedish = auto()
    Tamil = auto()
    Turkish = auto()

class PyTokenKind(Enum):
    Word = auto()
    StopWord = auto()
    SeparatorHard = auto()
    SeparatorSoft = auto()
    Unknown = auto()

class Region(Enum):
    BritishEnglish = auto()
    AmericanEnglish = auto()
    AustralianEnglish = auto()
    NewZealandEnglish = auto()
    CanadianEnglish = auto()
    IrishEnglish = auto()
    IndianEnglish = auto()
    SouthAfricanEnglish = auto()
    ScottishEnglish = auto()
    AustrianGerman = auto()
    SouthGerman = auto()
    NorthGerman = auto()
    EastGerman = auto()
    SwissGerman = auto()
    Regional = auto()
    MiddleWestGerman = auto()
    SouthWestGerman = auto()
    NorthWestGerman = auto()
    BadenWuerttembergGerman = auto()
    MiddleEastGerman = auto()
    SouthEastGerman = auto()
    NorthEastGerman = auto()
    MiddleGerman = auto()
    BavarianGerman = auto()
    NorthernIrish = auto()
    UpperGerman = auto()
    EastAustrianGerman = auto()
    BerlinGerman = auto()
    SwabianGerman = auto()
    WestAustrianGerman = auto()
    ViennaGerman = auto()
    TyrolGerman = auto()
    NorthEnglish = auto()
    DDRGerman = auto()
    PfalzGerman = auto()
    SouthTyrolGerman = auto()
    EastMiddleGerman = auto()
    SouthEastAsianEnglish = auto()
    HesseGerman = auto()
    LuxenbourgGerman = auto()
    WelchEnglish = auto()
    RhinelandPalatinateGerman = auto()
    SaxonyGerman = auto()
    WestGerman = auto()
    LiechtensteinGerman = auto()
    WestphaliaGerman = auto()
    SouthEastAustrianGerman = auto()
    NorthEastAustrianGerman = auto()
    NorthWestAustrianGerman = auto()
    SouthWestAustrianGerman = auto()
    WestSwissGerman = auto()
    NorthIrishEnglish = auto()
    MiddleAustrianGerman = auto()
    FranconianGerman = auto()
    EastSwissGerman = auto()
    Germanic = auto()

class Register(Enum):
    r"""
    In sociolinguistics, a register is a variety of language used for a particular purpose or particular communicative situation
    """
    Humor = auto()
    Vulg = auto()
    Techn = auto()
    Coll = auto()
    Geh = auto()
    Slang = auto()
    Iron = auto()
    Formal = auto()
    Euphem = auto()
    Literary = auto()
    Dialect = auto()
    Archaic = auto()
    Rare = auto()
    Pejorativ = auto()
    Figurative = auto()
    AlsoFigurative = auto()
    SpellingVariant = auto()
    Admin = auto()
    Transfer = auto()
    NetJargon = auto()
    Informal = auto()
    QuantityInformation = auto()
    IATEPreferred = auto()
    Miss = auto()
    ComGend = auto()

def create_topic_model_specific_dictionary(dictionary:PyDictionary,vocabulary:PyVocabulary) -> PyDictionary:
    ...

def read_aligned_articles(path,with_pickle = ...) -> PyAlignedArticleIter:
    ...

def read_aligned_parsed_articles(path,with_pickle = ...) -> PyAlignedArticleParsedIter:
    ...

def read_and_parse_aligned_articles(path,processor,with_pickle = ...) -> PyParsedAlignedArticleIter:
    ...

def read_and_parse_aligned_articles_into(path_in,path_out,processor,filter = ...,store_options = ...,with_pickle = ...) -> int:
    ...

def translate_topic_model(topic_model,dictionary,voting,config,provider = ...,voting_registry = ...) -> PyTopicModel:
    ...

